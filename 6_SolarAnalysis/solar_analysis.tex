\chapter{Solar Oscillation Analysis}\label{chap:solar_osc_analysis}
\epigraph{\textit{Driving out into the Sun\\Let the ultraviolet cover me up\\Looking for a Creation Myth\\Ended up with a pair of black lips}}{\textit{This is the End}\\ \textsc{Phoebe Bridgers}}
Measuring the ``solar'' neutrino oscillation parameters \dmsq{} and \tonetwo{} is one of the principal aims of the SNO+ detector during the scintillator phase. There are, in fact, two complementary methods of measuring these parameters: the oscillations of anti-neutrinos from terrestrial nuclear reactors, and the oscillations of neutrinos from the Sun.

This chapter focuses on the latter approach, using \beight{} neutrinos coming from the Sun to measure the solar oscillation parameters. An initial background-free study was performed by Javi Caravaca~\cite{caravacaSNOSensitivityStandard2020}, % Javi's tech note
which made an initial estimate of the sensitivity of SNO+. The work in this chapter builds substantially from that analysis. This chapter also draws on the associated reactor anti-neutrino analysis built by Iwan Morton-Blake~\cite{morton-blakeFirstMeasurementReactor2021}, % Iwan's thesis
and more broadly from the general techniques used in the \onbb{} analysis of Tereza Kroupova~\cite{kroupovaImprovingSensitivityNeutrinoless2020} and Jack Dunger~\cite{dungerTopologicalTimeBased2018}.% Tereza & Jack's thesis

This chapter begins by explaining how it is possible to measure the solar oscillation parameters via \beight{} events. Then, the framework used to perform the analysis is explained: that of a \textit{Bayesian Analysis using Markov Chain Monte Carlo techniques}. After the method has been described, the dataset upon which the analysis is performed is introduced. The results are then given. Given these results, a projection is then made for the expected sensitivity to \tonetwo{} as a function of livetime.


% \section{Analysis Methodology}\label{sec:analysis_method}
\section{Observational Principle}\label{sec:solar_obs_principle}
There are three key properties of solar neutrinos and their interactions that enable neutrino oscillations to be measured. The first of these is the dependence of the electron neutrino survival probability, $P_{ee}$, on both the individual oscillation parameters and the neutrino energy. As discussed in Section~\ref{sec:nu_osc_phenom}, $P_{ee}$ is dependent on the oscillation parameters \tonetwo{} and $\theta_{13}$ when propagating through the vacuum of space (after averaging over very many oscillations), and the effective oscillation parameters $\theta_{12}^{M}$ and $\Delta m^{2}_{M,21}$ when passing through the Sun or Earth. By Eq.~\ref{eq:matter_osc_params}, this leads to an overall dependence of $P_{ee} = P_{ee}\left(\theta_{12}, \theta_{13}, \Delta m^{2}_{12}\right)$.

Fig.~\ref{fig:pee_osc_param_dependence} shows the dependence of $P_{ee}(E)$ on each of these oscillation parameters, as well as the oscillation parameter $\Delta m^{2}_{13}$ for comparison. It is clear that only the parameters \dmsq{} and \tonetwo{} have a substantial impact on $P_{ee}(E)$. $\theta_{13}$ does have a small effect on the survival probability, but the global fit measurement of $\sin^{2}\theta_{13} = 0.02220^{+0.00068}_{-0.00062}$~\cite{estebanFateHintsUpdated2020} has uncertainties over an order of magnitude smaller than the range of $\sin^{2}\theta_{13}$ values scanned over in Fig.~\ref{fig:pee_osc_param_dependence}. Because of this, only \dmsq{} and \tonetwo{} will be varied in this analysis, with $\theta_{13}$ and $\Delta m^{2}_{13}$ being fixed at the global fit values shown in Table~\ref{tab:nufit_osc_params}.

% Let us now consider the dependence of $P_{ee}$ on the individual neutrino oscillation parameters. Recall from Eq.~\ref{eq:pee_msw} %
% that, after considering matter-induced oscillations due to neutrinos passing through the Sun and possibly the Earth, $P_{ee} = P_{ee}\left(\tan2\theta^{M}_{12}, \sin\theta^{M}_{13}, \Delta m^{2}_{21,M}\right) = P_{ee}\left(\theta_{12}, \theta_{13}, \Delta m^{2}_{12}, \Delta m^{2}_{13}\right)$. Fig.~\ref{fig:pee_osc_param_dependence} shows the dependence of each of these four oscillation parameters on $P_{ee}(E)$. We can see that in reality only the two parameters \dmsq{} and \tonetwo{} have a substantial impact on $P_{ee}(E)$ and hence the observed electron energy spectrum. Because of this, for this analysis we will only ever vary these two oscillation parameters, and keep $\theta_{13}$ and $\Delta m^{2}_{13}$ at their current ``NuFit 5.1'' global fit values\footnote{We use the global fit results excluding Super-Kamiokande's atmospheric data, and assuming normal ordering of the neutrino mass hierarchy. This choice has a tiny impact on the magnitudes of these two fixed parameters, the main impact being the sign of $\Delta m^{2}_{13}$.} % Confirm that mass hierarchy has no impact on Pee.
% of $\sin^{2}\theta_{13} = 0.0222$ and $\Delta m^{2}_{13} = +\SI{2.515e-3}{\eV\squared}$~\cite{estebanFateHintsUpdated2020}.% nufit citation

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/pee_scan_plots.pdf}
    \caption[$P_{ee}$ as a function of true neutrino energy, scanned over a variety of oscillation parameter values]
    {$P_{ee}$ as a function of true neutrino energy, scanned over a variety of oscillation parameter values. For a given oscillation parameter being scanned over, all other oscillation parameters are set at the NuFit 5.1 global fit values. $P_{ee}$ values are calculated using the \texttt{PSelmaa} algorithm, as described in Section~\ref{sec:osc_in_fit}.}
    \label{fig:pee_osc_param_dependence}
\end{figure}

This dependence of $P_{ee}$ on the oscillation parameters is not enough on its own to allow \dmsq{} and \tonetwo{} to be measured. If a purely NC weak interaction was used to detect the solar neutrinos, then there would be no way of telling the flavour-state of an interacting neutrino. The second property that enables solar neutrino oscillation measurements is that of neutrino-electron elastic scattering, the interaction mechanism used in this analysis. As discussed in Section~\ref{sec:nu_osc_evidence}, this interaction has an additional CC mode for electron neutrinos only, on top of an NC mode for all flavours of neutrino. This modifies the differential cross-section for electron neutrinos compared to other neutrino flavours. As a result, the resulting differential interaction rate $R$ for solar neutrinos in the detector as a function of the neutrino energy $E_{\nu}$, is given by:
% As discussed in Section~\ref{sec:nu_osc_evidence}, % link to theory section on solar oscillations
% it is possible to detect all flavours of neutrino through elastic scattering with electrons in the detector. Neutrinos can interact with the nuclei also, but generally the cross-sections of these interactions are orders of magnitude weaker than neutrino-electron elastic scattering~\cite{}, % CITE SOMETHING!
% so are not considered in this analysis. If the neutrino-electron elastic scattering interaction was purely neutral-current, then there would be no way of telling the flavour-state of an interacting neutrino. However, electron neutrinos are able to interact through an additional charged-current mode, which modifies the cross-section for electron neutrinos. The Sun only generates electron neutrinos, so as the survival probability for electron neutrinos ($P_{ee}$) is modified due to a change in the neutrino oscillation parameters, the interaction probability of neutrinos with the detector will also. The resulting differential interaction rate $R$ for solar neutrinos in the detector as a function of the neutrino's energy, $E_{\nu}$, is given by:
\begin{equation}\label{eq:solar_rate}
    \frac{dR}{dE_{\nu}} = 
        \Phi_{\beight{}}S_{\nu}\left(E_{\nu}\right)n_{e}
            \left[
                P_{ee}\left(E_{\nu}\right)\sigma_{\nu_e}\left(E_{\nu}\right) +
                \left(1-P_{ee}\right)\left(E_{\nu}\right)\sigma_{\nu_\mu,\tau}\left(E_{\nu}\right)
            \right],
\end{equation}
where $\Phi_{\beight{}}$ is the total flux of \beight{} solar neutrinos, $S_{\nu}$ is the normalised incident energy spectrum of the solar neutrinos for all flavours, $n_{e}$ is the number of electron targets in the detector detection medium, and $\sigma_{\nu_{i}}$ is the neutrino-electron elastic scattering cross-section for flavour $i$.

Of course, neutrino energies are not directly measured in the detector --- only the associated scattered electron. The final property needed to observe solar neutrino oscillations is a correlation between the energies of the incident neutrino and the scattered electron. Using the formula in Eq.~\ref{eq:enu_es_xsec}, the differential cross-sections of electron neutrinos at different incident energies, as a function of the scattered electron's kinetic energy is shown in Fig.~\ref{fig:nu_elec_energy_dependence}. For a given value of $E_{\nu}$, conservation of energy limits the maximum allowed kinetic energy of the scattered electron, $T_{\mathrm{max}}$, to be:
\begin{equation}
    T_{\mathrm{max}} = \frac{2E_{\nu}^{2}}{m_{e}+2E_{\nu}}.
\end{equation}
This kinematic limit can be very clearly seen in the shapes of the differential cross-sections. As a result, a higher energy neutrino will generate a higher energy scattered electron, on average. Fig.~\ref{fig:nu_elec_energy_dependence} also demonstrates that the correlation between neutrino and electron energies is rather weak.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/neutrino_electron_elastic_scattering_diff_xsecs.pdf}
    \caption[differential cross-section for neutrino-electron elastic scattering of electron neutrinos as a function of $T$ and $E_{\nu}$]
    {Plot showing the differential cross-section for neutrino-electron elastic scattering of electron neutrinos, $\frac{d\sigma_{\nu_{e}}}{dT}$, as a function of the kinetic energy of the scattered electron, $T$. This is shown for a variety of different incident neutrino energies.}
    \label{fig:nu_elec_energy_dependence}
\end{figure}

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/b8_energy_evolution_new.pdf}
    \caption[The evolution of energy distributions related to \beight{} solar neutrino detection]{The evolution of energy distributions related to \beight{} solar neutrino detection. The unoscillated neutrino spectrum is taken from~\cite{winterB8NeutrinoSpectrum2006}; % cite relevant SSM paper RAT uses
    neutrino oscillations assume oscillation parameters from the current global fit results. The latter three distributions were obtained from MC production as described in Section~\ref{sec:solar_dataset_livetime}.% Bahcall radiative corrections paper
    }
    \label{fig:nu_elec_energy_dependence2}
\end{figure}

The combined effect that each of the above physical processes has on the energy spectrum of the solar neutrinos can be seen in Fig.~\ref{fig:nu_elec_energy_dependence2}. A broad energy distribution of \beight{} electron neutrinos are generated in the Sun. These neutrinos then oscillate their flavour state as they propagate to the detector, in an energy-dependent manner. When neutrinos interact with the electrons in the detector, there is both an energy- and flavour-dependence on the cross-section. The scattered electrons gain a kinetic energy with some mild dependence on the inciting neutrino's energy, which is then measured by the detector to within some energy resolution.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/rec_energy_dist_vs_osc_params2.pdf}
    \caption[Comparison of the reconstructed electron energy distribution expected for 9 different oscillation parameter combinations]
    {Comparison of the reconstructed electron energy distribution expected for 9 different oscillation parameter combinations, after 1 year of data. The unoscillated rate was calculated in Appendix~\ref{chap:appendix_solar_rates}, with the cuts described in Section~\ref{sec:event_selection} being applied.}
    \label{fig:rec_energy_dist_vs_osc_params}
\end{figure}

The overall effect of changing the oscillation parameters on the observed signal distribution can be seen in Fig.~\ref{fig:rec_energy_dist_vs_osc_params}. There is some shape change in the spectrum as the oscillation parameters are modified. However, the correlation between the neutrino and electron energies is only weak, meaning the total rate of \beight{} events observed changing is the more dominant effect.


\begin{figure}[!th]
    \centering
    \begin{subfigure}{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{6_SolarAnalysis/images/nufit51_t12_dmsq21_solar_antinu.png}
        \caption{}
        \label{fig:nufit_osc_param_contours}
    \end{subfigure}
    \begin{subfigure}{0.54\textwidth}
        \centering
        \includegraphics[width=\textwidth]{6_SolarAnalysis/images/javi_solar_sensitivity.png}
        \caption{}
        \label{fig:javi_osc_param_contours}
    \end{subfigure}
    \caption[Solar oscillation parameter contours, from both the NuFit 5.1 global fit, and initial SNO+ sensitivity study]
    {\textbf{(a):} NuFit 5.1 global fit contours for the two solar neutrino oscillation parameters, after marginalisation over all other oscillation parameters~\cite{estebanFateHintsUpdated2020}. Contours correspond to the allowed regions at $1\sigma$, 90\%, $2\sigma$, 99\%, and $3\sigma$ confidence levels. \textbf{(b):} Sensitivity contours produced by J. Caravaca~\cite{caravacaSNOSensitivityStandard2020} for 1 year of SNO+ data in a solar oscillation analysis sensitivity study. The analysis assumed no backgrounds, systematics, or uncertainty on the \beight{} flux. The analysis took the NuFit 4.1 `solar' combined fit oscillation parameters to be true, and compared them to the reactor anti-neutrino measurement made by KamLAND~\cite{gandoReactorOnoffAntineutrino2013}.}
    \label{fig:previous_solar_osc_results}
\end{figure}

The NuFit 5.1 global fit results, shown in Table~\ref{tab:nufit_osc_params}, have the solar oscillation parameters measured as $\dmsq = 7.42^{+0.21}_{-0.20}\times10^{-5}\si{\eV\squared}$ and $\tonetwo = 33.44^{\circ+0.77^{\circ}}_{\phantom{\circ}-0.74}$~\cite{estebanFateHintsUpdated2020}. Fig.~\ref{fig:nufit_osc_param_contours} %
shows the current allowed parameter regions associated with this fit. Thanks to the work of both solar neutrino and reactor anti-neutrino oscillation experiments, both of these oscillation parameters are known to a precision better than 3\%. For comparison, the initial sensitivity study by J. Caravaca~\cite{caravacaSNOSensitivityStandard2020} gave an expected result for 1 year of data in the SNO+ scintillator phase shown in Fig.~\ref{fig:javi_osc_param_contours}. Note that this analysis assumed no backgrounds, systematics, or parameter constraints, and used a different statistical approach and set of analysis cuts to the ones described in the rest of this chapter. The study showed that, on its own, a solar oscillation analysis was unlikely to substantially improve the global fit measurement. However, it is possible that the solar analysis combined with a complementary reactor anti-neutrino analysis could do so. In any case, SNO+ has the capability to be the first experiment to measure \tonetwo{} and \dmsq{} using solar neutrino and reactor anti-neutrino sources in the same detector. Moreover, it is valuable to perform a more detailed study of the solar analysis in SNO+, incorporating backgrounds, systematics, and constraints. This is the work of the rest of this chapter.

\section{Background Processes}\label{sec:background_processes}
There are a number of background processes that the solar signal must compete against. Below a scattered electron reconstructed energy of $\sim\SI{2.5}{\MeV}$, it is known that low-energy backgrounds coming mainly from Uranium- and Thorium-chain isotopes coming from the natural radioactivity of the detector materials completely dominate over the \beight{} signal, and so for this analysis only processes that can generate reconstructed energies of at least $E_{\textrm{min}} = \SI{2.5}{\MeV}$ are considered. The following subsections explain each of the backgrounds present in the dataset above $E_{\mathrm{min}}$, as well as methods that have been used to mitigate them as much as possible.

\subsubsection{Internal Uranium- and Thorium-Chain Backgrounds}\label{sec:u_th_internals}
Although every effort has been made to make the scintillator cocktail that fills SNO+ to be as radio-pure as possible, there inevitably remain trace amounts of the radioactive isotopes that derive from the decay chains of the \ce{^{238}U} and \ce{^{232}Th} isotopes. Fig.~\ref{fig:u_th_decay_chains} shows these two decay chains. Only a fraction of the radioactive isotopes in these chains actually are capable of generating events in the detector with energies above $E_{\textrm{min}}$: these have been highlighted in Fig.~\ref{fig:u_th_decay_chains} in gold.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/U238_Th232_decay_chains_tereza.png}
    \caption[The \ce{^{238}U} and \ce{^{232}Th} decay chains.]{The \ce{^{238}U} and \ce{^{232}Th} decay chains, taken from~\cite{kroupovaImprovingSensitivityNeutrinoless2020}. % Tereza's thesis
    Isotopic half-lives are given below their symbol; the Q-values for each decay, in MeV, is given in green. Downward arrows indicate an $\alpha$-decay; diagonal arrows indicate $\beta$-decay. Isotopes highlighted in gold are potential backgrounds for this solar analysis.}
    \label{fig:u_th_decay_chains}
\end{figure}

Of particular note are the decays of \ce{^{212}Bi} and \ce{^{214}Bi}, both of which can either $\alpha-\beta$ decay via \ce{Tl}, or $\beta-\alpha$ decay via \ce{Po}. For the former, it is the subsequent $\beta$-decay of the \ce{Tl} that can have a reconstructed energy above $E_{\textrm{min}}$. For the latter, the \ce{Bi} decay is the part of the pair of decays that can lie above $E_{\textrm{min}}$. Although the $\alpha$-decays here certainly have Q-values well above \SI{2.5}{\MeV}, the liquid scintillator quenches the observed energy to well below \SI{2.5}{\MeV}. The so-called ``\ce{Bi-Po}'' decays are particularly special because the lifetimes of \ce{^{212}Po} and \ce{^{214}Po} are \SI{300}{\nano\second} and \SI{164}{\micro\second}, respectively, which are short enough to allow for highly-effective coincidence tagging of these events.

\nomenclature{\textbf{OOW}}{Out-of-window events}
\nomenclature{\textbf{IW}}{In-window events}
There are two main classes of \ce{Bi-Po} event in the detector: ``out-of-window'' (OOW) events for which the \ce{Bi} $\beta$-decay and \ce{Po} $\alpha$-decay events occur in separate triggered events, and ``in-window'' (IW) events whereby the \ce{Bi} and \ce{Po} occur within the same event. These lead to two distinct strategies for tagging these kinds of events. For out-of-window \ce{Bi-Po}s, a delayed coincidence of two events is searched for in the same position: it is assumed that in the short time between the creation and decay of the daughter nucleus, there is negligible movement relative to the mother nucleus. Using the tagging algorithm suggested in~\cite{wilsonBiPoRejectionFactors2017,kroupovaImprovingSensitivityNeutrinoless2020} % Jeanne's BiPo tagging DocDB
as a starting point, the chosen procedure is described in Table~\ref{tab:bipo_oow_tagging_requirements}. This choice of cuts was designed to be very broad, to ensure that the tagging was as \textit{efficient} in rejecting \ce{Bi-Po}s as possible, whilst negligibly impacting the solar signal. This is in contrast to the cuts chosen by Rafael Hunt-Stokes in~\cite{hunt-stokesUraniumThoriumBackground2022}, % Raf's BiPo tagging tech note on DocDB
which uses a much tighter cut on position as well as cuts on the energies of the prompt and delayed events, to try and obtain a highly \textit{pure} sample of \ce{Bi-Po} tags.

\begin{table}
    \centering
    \begin{tabular}{c >{\centering\arraybackslash}m{7.5 cm}}
        \hline
        Property      & Requirement \\ \hline \hline
        Prompt Event  & Triggers detector, valid position reconstruction \\
        Delayed Event & Triggers detector, valid position reconstruction, $\texttt{nhitsCleaned} \geq 100$ \\
        $\Delta t$    & $<\SI{4}{\milli\second}$ \\
        $\Delta R$    & $<\SI{2}{\metre}$ \\
        \hline
    \end{tabular}
    \caption[Summary of cuts used for coincidence tagging]
    {Summary of cuts used for coincidence tagging.}
    \label{tab:bipo_oow_tagging_requirements}
\end{table}

The above delayed coincidence procedure cannot catch any of the IW \ce{Bi-Po} events. For these, a different approach is used. Because two decays happened in the same event, two distinct peaks in the event's time residual spectrum are expected to be seen. In order to look for this event topology, a likelihood-ratio classifier was run over events, first developed by Eric Marzec~\cite{marzecBiPoEventReduction2013} % Eric's tech note on DocDB
and re-coordinated for the \SI{2.2}{\gpl} LAB-PPO scintillator optics by Ziping Ye~\cite{yeTaggingInwindowBiPo2022}. % Ziping's presentation on DocDB
This classifier calculates the likelihood ratio between the null hypothesis of a \onbb{} event (a proxy in this analysis for single-decay events such as the \beight{} signal) and the alternative hypothesis of an IW \ce{Bi-Po} event. The results of this classifier are shown in Fig.~\ref{fig:bipo_tagging_IW}. As can be seen, the more negative the value of the result, \texttt{alphabeta212}, the greater the evidence there is for rejecting the null hypothesis of a single-site event. Events with $\texttt{alphabeta212} < 0$, % WRITE FINAL CHOICE OF CUT!
or the equivalent for \ce{^{214}Bi-Po} events, \texttt{alphabeta214}, were then rejected.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/bipo_tagging_1d.pdf}
    \caption[The distributions of the IW \ce{^{212}Bi-Po} classifier for \ce{Bi-Po} and \beight{} events in simulation, within the analysis ROI]
    {The distributions of the IW \ce{^{212}Bi-Po} classifier for \ce{Bi-Po} and \beight{} events in simulation, within the analysis ROI. The cut of $\texttt{alphabeta212} < 0$ clearly removes a large fraction of \ce{^{212}Bi-Po} events, and 2\% of \ce{^{214}Bi-Po} events, while keeping 99.89\% of the \beight{} signal. The relative normalisations before these cuts have been applied is arbitrary.}
    \label{fig:bipo_tagging_IW}
\end{figure}

\nomenclature{\textbf{ROI}}{Region of Interest (of a physics analysis)}
Combining both OOW and IW \ce{Bi-Po} tagging, the impact on \ce{^{212}Bi-Po}, \ce{^{214}Bi-Po}, and \beight{} $\nu_e$ events can be seen in Fig.~\ref{fig:bipo_tagging_efficiency}. Only events that pass all other cuts used in this analysis (other than the cuts for externals defined shortly) are considered: these will be explained in Section~\ref{sec:event_selection}. % link to cuts subsection
Because of the different lifetimes of the decays relative to the length of the event trigger window, \ce{^{214}Bi-Po} decays predominantly fall out-of-window whilst \ce{^{212}Bi-Po} events are typically in-window. This explains why the out-of-window tagging is substantially better at cutting \ce{^{214}Bi-Po} decays, whereas the in-window tagging far better tags \ce{^{212}Bi-Po} decays. Overall, within the analysis region of interest (ROI), the two combined cuts are able to tag 99.77\% % CALCULATE
of \ce{^{214}Bi-Po} triggered events, 94.84\% % CALCULATE
of \ce{^{212}Bi-Po} triggered events, whilst retaining 99.85\% % CALCULATE
of \beight{} $\nu_e$ signal events.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/bipo_tagging_eff.pdf}
    \caption[\ce{^{214}Bi-Po}, \ce{^{212}Bi-Po}, and \beight{} observed energy spectra before and after OOW and IW cuts]{\ce{^{214}Bi-Po}, \ce{^{212}Bi-Po}, and \beight{} observed energy spectra in MC before and after OOW and IW cuts. The relative normalisations before these cuts have been applied is arbitrary.}
    \label{fig:bipo_tagging_efficiency}
\end{figure}

\subsubsection{\alphan{} Reactions}\label{sec:alphans}
The impact of \ce{^{238}U}- and\ce{^{232}Th}-chain isotopes does not simply end at their direct decays. It is possible for the $\alpha$s generated during these decays to undergo their own interactions with nuclei in the detector. Within the organic scintillator of SNO+, the dominant interaction of this type is when an $\alpha$ collides with a \ce{^{13}C} nucleus, emitting a neutron: \ce{\alpha{} + ^{13}C -> ^{16}O + n}. This is known as an \alphan{} reaction.

The topology of this reaction in the detector is a delayed coincidence, as shown in Fig.~\ref{fig:alpha_n_drawing}. % Iwan's alpha-n sketch
The prompt signal can be generated through a number of processes, including the decay of an excited \ce{^{16}O} state. The neutron generated in the interaction then thermalises and gets captured by another nucleus --- usually hydrogen in SNO+ --- which creates an excited state that then eventually decays, creating a \ce{\gamma} that Compton scatters to create the delayed signal in the detector.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{6_SolarAnalysis/images/alpha_n_schematic_Iwan_modified.pdf}
    \caption[Schematic of \alphan{} interactions]
    {Schematic of the three dominant modes of \alphan{} interaction, taken from~\cite{morton-blakeFirstMeasurementReactor2021}. % Iwan's thesis
    }
    \label{fig:alpha_n_drawing}
\end{figure}

As can be seen in Fig.~\ref{fig:alpha_n_coincidence_cut_impact}, \alphan{} interactions can lead to events reconstructed at a wide variety of energies, which could be an issue for this analysis. However, because they are delayed coincidence events with a typical decay time of $\sim\SI{100}{\nano\second}$, the aforementioned out-of-window and in-window \ce{Bi-Po} tagging algorithms also efficiently tag \alphan{} events. Looking again at Fig.~\ref{fig:alpha_n_coincidence_cut_impact}, simply by using the out-of-window and in-window \ce{Bi-Po} taggers without any further modifications 99.37\% of events in the ROI are cut.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{6_SolarAnalysis/images/alphan_tagging_eff.pdf}
    \caption[\alphan{} reconstructed energy spectrum in MC within the analysis ROI, before and after out-of-window cuts have been applied]
    {\alphan{} reconstructed energy spectrum in MC within the analysis ROI, before and after out-of-window cuts have been applied. The relative normalisation of the distribution before the additional cuts have been applied is arbitrary.}
    \label{fig:alpha_n_coincidence_cut_impact}
\end{figure}


\subsubsection{External Backgrounds}
To distinguish between the inherent backgrounds within the scintillator, and the backgrounds from materials at larger radii, the terminology ``internal'' and ``external'', is used, respectively. External backgrounds can come from the acrylic, ropes, external water, and PMTs. These components have had their radiopurity measured throughout the detector's lifetime, often back to the construction of the original SNO detector itself. The materials other than the liquid scintillator are known to have far higher background levels, especially in the important \ce{^{238}U}- and \ce{^{232}Th}-chain backgrounds~\cite{andringaCurrentStatusFuture2016}. % good ref for this

\nomenclature{\textbf{FV}}{Fiducial Volume}
Although there are numerous external backgrounds, with an accurate and precise position reconstruction algorithm they can be efficiently rejected. The simplest approach is with a ``fiducial volume'' (FV) cut: all events that reconstruct beyond some radius are cut. The only external background events that will reach within the FV are those that have reconstructed very poorly, or have some long-distance radiation that manages to deposit radiation towards the centre of the AV. % FOM cut?
Because $\alpha$ and $\beta$ radiation can only travel short distances through the detector, it is only $\gamma$ % and neutron?
radiation that can realistically travel far enough into the detector to be able to reconstruct anywhere near the centre. Moreover, the intensity of this $\gamma$ radiation attenuates exponentially with the Compton scattering length towards the centre of the detector. This strong radial-dependence can be seen in Fig.~\ref{fig:external_radial_dependence}. One example of such a background are `PMT $\beta-\gamma$' events, in which the \ce{^{238}U}- and \ce{^{232}Th}-chain radio-isotopes present in the PMT photocathodes generate $\gamma$s from the decays.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{6_SolarAnalysis/images/externals_radial_plot.pdf}
    \caption[Radial dependence of external backgrounds relevant in this analysis, as compared to the \beight{} signal]
    {Radial dependence of external backgrounds relevant in this analysis, as compared to the \beight{} signal. Events for all processes shown here had to pass the first 6 cuts of Table~\ref{tab:ev_selection}.}
    \label{fig:external_radial_dependence}
\end{figure}

What this figure also demonstrates is that the solar signal has a completely different radial dependence to these backgrounds. As a result, if one considers not just the energy of events but also their reconstructed radius, then it is possible to get an additional handle on the external backgrounds. The FV cut can then be pushed further out to larger radii, allowing one to gain more signal statistics.

Work by Tereza Kroupova~\cite{kroupovaImprovingSensitivityNeutrinoless2020} % Tereza's thesis & full-scint re-coordination
allows for additional means of distinguishing external backgrounds from the solar signal. The underlying assumption in the reconstruction of SNO+ events is that photons were created from an electron which lost its kinetic energy in a near-point-like region of the scintillator. This is a reasonable assumption for \beight{} elastic scattering events. However, external backgrounds can fail this assumption in two ways. Firstly, these radioactive decays often generate $\gamma$ radiation in addition to the main $\alpha$/$\beta$ particle, which creates an event with energy depositions in multiple different positions: a ``multi-site'' event. Because the \texttt{scintFitter} position reconstruction algorithm is not designed to deal with multi-site events in the scintillator, the time residual distribution will change. This allows an event classifier to be built that distinguishes between the \tres{} distributions of single-site events and externals, known as the ``external background timing classifier'' (\texttt{ext0NuTimeTl208AVNaive}).

Secondly, external backgrounds should have the earliest PMT hits associated with PMTs closest to the main site of energy deposition. Because external events that reconstruct at small radii typically have a $\gamma$ that travelled a long distance towards the centre of the detector, the direction from the centre of the detector to the reconstructed position should point in the direction of the deposition from the $\gamma$, the main decay site, and hence the PMTs which were hit first. A distribution of PMT hits for a given event as a function of their angular distribution relative to this direction can be built, and compared to the expected distributions for single-site and external background events. This is known as the ``external background topological classifier'' (\texttt{ext0NuAngleTl208AV}).

Fig.~\ref{fig:external_classifier_corr} shows the correlation between the two classifier results for both \ce{^{208}Tl} decays in the acrylic, and \beight{} $\nu_{e}$ events, using MC. The other external backgrounds in MC have a similar distribution in the external classifiers to that of the \ce{^{208}Tl} decays in the acrylic.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{6_SolarAnalysis/images/externals_tagging_eff.pdf}
    \caption[Distributions of the two external classifiers, for both the \beight{} signal and \ce{^{208}Tl} decays in the acrylic]
    {Distributions of the timing and topological external classifiers, for both the \beight{} signal and \ce{^{208}Tl} decays in the acrylic. Contours indicate lines of equal probability density. All events shown were in the analysis ROI, and below \SI{5}{\MeV}. Grey regions indicate the cuts used in this analysis.}
    \label{fig:external_classifier_corr}
\end{figure}

As can be seen, the external background is far more spread out along both axes compared to the signal. This allows for discrimination of external backgrounds from the signal in the \SIrange{2.5}{5.0}{\MeV} range. For this analysis, a cut on both classifiers were used: to pass, an event required $\mathtt{ext0NuTimeTl208AVNaive} > -0.007$ and $\mathtt{ext0NuAngleTl208AV} > -4.7$. By using these cuts, 37.4\% of external acrylic \ce{^{208}Tl} were cut in the analysis ROI in MC, whilst keeping 99.3\% of the \beight{} $\nu_{e}$ signal.

% ~~~~~~~~~~~~~~~~~~~~~~~
% TODO IF TIME!!!!
% Look at systematics of classifiers, by comparing data to MC for certain
% control regions: tagged BiPos, high-r externals, high-E B8 evs.
%

% \subsubsection{Cosmogenic Isotopes}
% The final source of background events are radioactive isotopes that form via collisions of cosmic rays with atomic nuclei, known as cosmogenic isotopes. Most of these isotopes are short-lived~\cite{}, % cite Borexino/KamLAND(?) paper handling cosmogenics
% with lifetimes $\mathcal{O}(\SI{1}{\second})$. Fortunately, the depth of SNO+ means that our rate of cosmic ray muons interacting with the detector is only 3 an hour~\cite{}. % cite Lorna/Katharine; Q: confident only muons? What about other kinds of cosmic rays? Presumably nothing else can penetrate this far with high enough rates
% Because the rate is so low relative to other experiments, relatively straightforward approaches to tagging and removing cosmic ray muons and their cosmogenic followers can be utilised without substantial loss of livetime. Events are tagged as a cosmic ray muon if they create a sufficient number of hits in the outward-looking PMTs above background levels, as well as many hits within the detector itself. The details of this tagging for the scintillator-fill were put in place by Lorna Nolan~\cite{}, % Lorna's thesis(?); Katharine's contribution?
% modifying the existing algorithms used in the water-phase~\cite{} % Billy's Thesis
% and in SNO~\cite{}. % Appropriate SNO citation (old thesis?)
% After a tagged cosmic muon event, all events for the next \SI{20}{\second} are then vetoed as a means of rejecting followers. This simple cut is enough to remove the vast majority of expected cosmogenic events in the scintillator-phase. % CITATION?
% The expected impact on loss of livetime, and hence quantity of signal events, is 3 lots of \SI{20}{\second} vetoes an hour, that is to say 1/60\textsuperscript{th} of the signal is lost via this cut.

% There is one cosmogenic isotope with a long-enough half-life that even the \SI{20}{\second} muon follower veto is not sufficient to remove all events. This is \ce{^{11}C}, which \ce{\beta+}-decays to stable \ce{^{11}B} with a half-life of \SI{20.4}{\minute}. The maximum possible energy deposited in this decay is \SI{1.982}{\MeV}~\cite{}, % CITATION
% just below $E_{\textrm{min}}$, so only a small fraction of \ce{^{11}C} events end up in the ROI: the ones with very high energies that get their energy reconstruction falsely-inflated by some amount. As a result, this background is expected to be very much sub-dominant to all other backgrounds in this analysis. Because this background is important to consider for some other analyses, a triple-coincidence tagging algorithm is being built by Katharine Dixon~\cite{}, % Katharine C11 citation
% but not used for this analysis currently.% Maybe this changes? will see if constraint needed.

\section{Statistical Approach}
\subsection{The Log-likelihood Test Statistic}\label{sec:test_statistic}
At the highest level, this analysis involves taking the data observed in the scintillator-fill after applying a certain set of cuts, along with simulated PDFs for all processes believed to constitute the observed data with those same cuts applied. Using these, the combined energy and radial distributions of the MC are fit to that of the data. Given a set of PDFs, a number of parameters can be modified to try and match the distribution of observables in data. These consist of the normalisations of each PDF (i.e. the total number of events observed due to that process), and any systematic parameters that could modify the shapes of these distributions. For this analysis, the neutrino oscillation parameters act as \textit{de facto} systematic parameters, as they modify the shape of the \beight{} PDFs.

The test statistic used for this analysis is the \textit{binned extended log-likelihood}:
\begin{align}
    \mathcal{L}\left(\bm{\mathcal{N}}, \bm{\theta} | \bm{n}\right) &=
    -\sum_{i=1}^{N_{\mathrm{PDFs}}}\mathcal{N}_{i} 
    + \sum_{j=1}^{N_{\mathrm{bins}}}n_{j}\ln{\left(\sum_{i=1}^{N_{\textrm{PDFs}}} \mathcal{N}_{i}P_{ij}\left(\bm{\theta}\right)\right)},%\nonumber\\
    % &-\sum_{k=1}^{N_{\mathcal{N}'}}\frac{\left(\mathcal{N}_{k}' - \mathcal{N}_{k}^{\mathrm{nom}}\right)^2}{2(\sigma_{k}^{\mathcal{N}})^{2}}
    % -\sum_{\ell=1}^{N_{\theta'}}\frac{\left(\theta_{\ell}' - \theta_{\ell}^{\mathrm{nom}}\right)^2}{2(\sigma_{\ell}^{\theta})^{2}}.
\end{align}
where $\mathcal{N}_{i}$ is the normalisation parameter for the $i^{\mathrm{th}}$ PDF out of $N_{\mathrm{PDFs}}$ in total, $n_{j}$ is the number of observed events in the $j^{\mathrm{th}}$ bin of data out of $N_{\mathrm{bins}}$ total bins, and $P_{ij}\left(\bm{\theta}\right)$ is the probability of observing an event of type $j$ in bin $i$, assuming a set of systematic parameters $\bm{\theta}$.

% In order to perform a fit to data in this way, we must first answer a set of questions:
% \begin{enumerate}
%     \item Which signal and background processes must we consider?\label{q:signal_v_background}
%     \item In addition to their normalisations, are there any further parameters necessary to specify the distributions of the PDFs for each of the processes? Systematics and oscillation parameters are good examples.
%     \item What is our test statistic?\label{q:test_statistic}
%     \item What algorithm do we use to try and find the best-fit result?\label{q:algorithm}
%     \item How do we measure uncertainties on these best-fit values for each parameter?\label{q:uncertainty}
% \end{enumerate}
% In section~\ref{sec:background_processes}, question~\ref{q:signal_v_background} was answered for this analysis. We now give the answer to question~\ref{q:test_statistic}; all other questions on this list will be answered shortly.

% The test statistic used for this analysis is the \textit{binned extended log-likelihood.} Once the data and MC PDFs have been binned in both the observables of interest, it is assumed that the expected number of events in a given bin $j$ is governed by a Poisson distribution:
% \begin{equation}
%     P_{j}\left(n_{j} | \lambda_{j}\right) = \frac{\lambda_{j}^{n_{j}}e^{-\lambda_{j}}}{n_{j}!},
% \end{equation}
% where $P_{j}\left(n_{j} | \lambda_{j}\right)$ is the probability of observing $n_{j}$ events in bin $j$, given an expectation of $\lambda_{j}$ events in total from signal and background processes in that bin. This $\lambda_{j}$ can be decomposed into each of the expected rates for each process, $i$:
% \begin{equation}
%     \lambda_{j} = \sum_{i=1}^{N_{\textrm{PDFs}}} \mathcal{N}_{i}P_{ij}\left(\bm{\theta}\right),
% \end{equation}
% where $N_{\textrm{PDF}}$ is the number of PDFs being considered in the analysis, $\mathcal{N}_{i}$ is the normalisation parameter of the $i^{\textrm{th}}$ PDF, and $P_{ij}\left(\bm{\theta}\right)$ is the probability of observing an event of process type $j$ in bin $i$, assuming a set of non-normalisation parameters $\bm{\theta}$. By combining the probabilities of all the bins together, %and also adding the possibility of constraining any of the normalisation or systematic parameters $\bm{\mathcal{N}}$ or $\bm{\theta}$, 
% the total probability for a given set of processes assuming these parameters to give rise to the data seen is:
% \begin{align}
%     P\left(\bm{n} | \bm{\mathcal{N}}, \bm{\theta}\right) = 
%     L\left(\bm{\mathcal{N}}, \bm{\theta} | \bm{n}\right) &= 
%     \prod_{j=1}^{N_{\textrm{bins}}} \frac{\left[\sum_{i=1}^{N_{\textrm{PDFs}}} \mathcal{N}_{i}P_{ij}\left(\bm{\theta}\right)\right]^{n_{j}}e^{-\sum_{i=1}^{N_{\textrm{PDFs}}} \mathcal{N}_{i}P_{ij}\left(\bm{\theta}\right)}}{n_{j}!},
%     % \nonumber\\
%     % &\cdot\prod_{k=1}^{N_{\mathcal{N}'}}\frac{e^{-\frac{\left(\mathcal{N}_{k}' - \mathcal{N}_{k}^{\mathrm{nom}}\right)^2}{2(\sigma_{k}^{\mathcal{N}})^{2}}}}{\sqrt{2\pi\sigma_{k}^{\mathcal{N}}}}
%     % \cdot\prod_{\ell=1}^{N_{\theta'}}\frac{e^{-\frac{\left(\theta_{\ell}' - \theta_{\ell}^{\mathrm{nom}}\right)^2}{2(\sigma_{\ell}^{\theta})^{2}}}}{\sqrt{2\pi\sigma_{\ell}^{\theta}}}
% \end{align}
% where $N_{\mathrm{bins}}$ is the total number of bins being considered in the analysis. 
% % Gaussian constraints
% % \footnote{% note about Bayesian interpretation of constraints
% %     As we shall see in section~\ref{}, % reference to Bayesian analysis explanation section
% %     because this analysis uses a Bayesian statistical approach, it is more appropriate to talk of these constraints as non-uniform ``priors'' for the parameters.
% % } 
% % on a subset of the normalisations have been included, $\left\{\mathcal{N}'\right\}$, of which there are $N_{\mathcal{N}'}$ in total. For each of these normalisations, indexed by $k$, there is an associated nominal value $\mathcal{N}_{k}^{\mathrm{nom}}$ and width $\sigma_{k}^{\mathcal{N}}$. There are similar constraints on a subset of the systematic parameters, with similarly-named variables.
% This probability can be re-framed as the likelihood of the vectors of parameters $\bm{\mathcal{N}}$ and $\bm{\theta}$ given the vector of number of events in each bin, $\bm{n}$: $L\left(\bm{\mathcal{N}}, \bm{\theta} | \bm{n}\right)$. It is rare to see the likelihood as-is, instead, for computational purposes the log-likelihood is used instead, $\mathcal{L}\left(\bm{\mathcal{N}}, \bm{\theta} | \bm{n}\right) := \ln{L}\left(\bm{\mathcal{N}}, \bm{\theta} | \bm{n}\right)$. We can then get to the formula actually used for this analysis:
% \begin{align}
%     \mathcal{L}\left(\bm{\mathcal{N}}, \bm{\theta} | \bm{n}\right) &=
%     -\sum_{i=1}^{N_{\mathrm{PDFs}}}\mathcal{N}_{i} 
%     + \sum_{j=1}^{N_{\mathrm{bins}}}n_{j}\ln{\left(\sum_{i=1}^{N_{\textrm{PDFs}}} \mathcal{N}_{i}P_{ij}\left(\bm{\theta}\right)\right)}.%\nonumber\\
%     % &-\sum_{k=1}^{N_{\mathcal{N}'}}\frac{\left(\mathcal{N}_{k}' - \mathcal{N}_{k}^{\mathrm{nom}}\right)^2}{2(\sigma_{k}^{\mathcal{N}})^{2}}
%     % -\sum_{\ell=1}^{N_{\theta'}}\frac{\left(\theta_{\ell}' - \theta_{\ell}^{\mathrm{nom}}\right)^2}{2(\sigma_{\ell}^{\theta})^{2}}.
% \end{align}


\subsection{The Bayesian Statistical Approach}
There are two main schools of statistical inference, ``Frequentist'' and ``Bayesian''. In the former, probabilities describe the fraction of times a situation can be found within the whole ensemble of possible worlds. For the latter, what matters instead is our degree of belief about matters in this current one. Our beliefs are updated as knowledge is acquired about the world through Bayes' Theorem:
\begin{equation}
    P\left(\bm{\mu}|\bm{x}\right) = \frac{\mathcal{L}\left(\bm{\mu}|\bm{x}\right)P\left(\bm{\mu}\right)}{P\left(\bm{x}\right)}.
\end{equation}
Here, $\bm{\mu}$ is the set of parameters that model a given system, $P\left(\bm{\mu}\right)$ is our \textit{prior} (pre-existing) distribution for those model parameters, and $\bm{x}$ is the data taken in an experiment. The updated, \textit{posterior} distribution $P\left(\bm{\mu}|\bm{x}\right)$ is then the prior multiplied by the likelihood of parameters $\bm{\mu}$ given observations $\bm{x}$, $L\left(\bm{\mu}|\bm{x}\right)$, and divided by the total probability $P\left(\bm{x}\right)$ of observing $\bm{x}$ under any circumstance. It is the Bayesian approach that has been used in this analysis.

\nomenclature{\textbf{CI}}{Credible Interval}
\nomenclature{\textbf{HPD}}{(The point of) Highest Posterior Density}
If one is able to determine the overall posterior distribution, then it is possible to derive best-fit values with uncertainties for all parameters in the fit. This is done by ``marginalising'' the posterior distribution, i.e. integrating over all parameters other than the one of interest. A sensible best-fit value is then the point of Highest Posterior Density (HPD). The uncertainty on this value is derived from the spread of the marginalised posterior, by the calculation of a $1\sigma$ Credible Interval (CI): this is a set of values for a given parameter which has a total posterior probability of 68.3\%, and contains the best-fit value. There are an infinite number of CIs that satisfy this property; for this analysis, the values have been chosen in decreasing order of marginalised posterior probability density.

\subsection{Markov Chain Monte Carlo}
The above discussion of Bayesian statistics assumes that one can accurately determine the posterior density distribution. Whilst the likelihood and prior distribution are straightforward enough to calculate, often-times $P\left(\bm{x}\right)$ (which acts as a normalisation) is very challenging to determine. This is because calculating this normalisation involves integrating the likelihood over all the parameter space, and if there are a large number of parameters this can become very numerically complex.

An alternative approach comes in the form of \textit{Markov Chain Monte Carlo}, MCMC. A Markov Chain is any mathematical system for which the next state of the system is dependent only on its current state; the system is in some sense ``memoryless''. For a large class of Markov Chains --- those that are ``ergodic'' and ``aperiodic'' --- one can prove that regardless of the initial position on the chain, the distribution of possible positions that the system can be in converges~\cite{fellerIntroductionProbabilityTheory1968}. % CITE Feller, W. (1968). An Introduction to Probability Theory and its Applications
MCMC uses such a Markov Chain which attempts to converge towards the posterior density distribution in particular. In MCMC, after choosing the initial position in the parameter space, successive states are chosen at random with a probability dependent only on the properties of the current position in parameter space and the proposed position. It is possible to show that, because of the convergence property of Markov chains, the set of steps made in the parameter space will have a distribution that converges to that of the posterior density distribution~\cite{fellerIntroductionProbabilityTheory1968}.

The MCMC algorithm used in this analysis is that of the \textit{Random-Walk Metropolis Algorithm}. In this algorithm, given an initial position in the parameter space $\bm{\mu}$, a new position is proposed, $\bm{\mu}'$. This position is chosen at random from a multivariate Gaussian distribution centred on the current position, with widths in each dimension of the parameter space chosen beforehand as constants. These widths are chosen through tuning the MCMC process. This choosing of a new proposed step at random is what gives the algorithm its Monte Carlo and Random Walk titles. Once a new position is proposed, it is accepted as the new position with a probability $S\left(\bm{\mu}'|\bm{\mu}\right)$ according to the condition of \textit{detailed balance}:
\begin{align}
    S\left(\bm{\mu}'|\bm{\mu}\right) &= 
    \min{\left(
        1,
        \frac{P\left(\bm{\mu}'|\bm{x}\right)}{P\left(\bm{\mu}|\bm{x}\right)}
        % \frac{R\left(\bm{\mu}|\bm{\mu}'\right)}{R\left(\bm{\mu}'|\bm{\mu}\right)}
        \right)
        } = 
    \min{\left(
        1,
        \frac{L\left(\bm{\mu}'|\bm{x}\right)P\left(\bm{\mu}'\right)}{L\left(\bm{\mu}|\bm{x}\right)P\left(\bm{\mu}\right)}
        % \frac{R\left(\bm{\mu}|\bm{\mu}'\right)}{R\left(\bm{\mu}'|\bm{\mu}\right)}
        \right)
    }\nonumber\\
    &= \min{\left(
        1,
        % \frac{R\left(\bm{\mu}|\bm{\mu}'\right)}{R\left(\bm{\mu}'|\bm{\mu}\right)}
        \exp{\left[\mathcal{L}\left(\bm{\mu}'|\bm{x}\right)-\mathcal{L}\left(\bm{\mu}|\bm{x}\right)+\ln{\frac{P\left(\bm{\mu}'\right)}{P\left(\bm{\mu}\right)}}\right]}
        \right)
    }.
\end{align}
% $R\left(\bm{\mu}'|\bm{\mu}\right)$ is the probability density that position $\bm{\mu}'$ is proposed as a step from position $\bm{\mu}$, and vice versa for $R\left(\bm{\mu}|\bm{\mu}'\right)$. In most cases, because of the use of the same multivariate Gaussian in choosing proposals, $\frac{R\left(\bm{\mu}|\bm{\mu}'\right)}{R\left(\bm{\mu}'|\bm{\mu}\right)} = 1$ simply. This component only becomes important at the edges of the parameter space, preventing the sampling probability being incorrectly impacted if a proposed step goes outside the allowed parameter space.

It is the detailed balance condition that ensures convergence of the MCMC algorithm to specifically the posterior density distribution. Crucially, because it is only dependent on the ratio of posterior densities, the hard-to-calculate normalisation $P\left(\bm{x}\right)$ in both posterior density terms cancels out, meaning one only needs to calculate the likelihood and priors for each step.
%  as well as $\frac{R\left(\bm{\mu}|\bm{\mu}'\right)}{R\left(\bm{\mu}'|\bm{\mu}\right)}$.

The specific implementation of MCMC used for this analysis is that of \texttt{OXO}, a C++ analysis framework first developed by Jack Dunger~\cite{dungerTopologicalTimeBased2018} and extended for this analysis. % Jack's thesis
\texttt{OXO} is able to run the Metropolis algorithm on multidimensional binned data, using the log-likelihood defined in Section~\ref{sec:test_statistic}. This framework also allows one to include systematic parameters that can float within the fit, and define non-uniform priors for normalisations and systematics that have constraints.

\subsection{Choosing Priors}
For this analysis, the suggestions made by Biller \& Oser in~\cite{billerAnotherLookConfidence2015} % Steve's stats paper 
about choosing prior distributions are followed: for parameters that do not have some pre-existing constraint, a flat prior is used. A nice consequence of this choice is that $\ln{\frac{P\left(\bm{\mu}'\right)}{P\left(\bm{\mu}\right)}} = 0$, so the actual value of the prior for these variables never needs to be calculated when running the MCMC algorithm. For this analysis, uniform priors are assumed on the neutrino oscillation parameters \dmsq{} and \tonetwo{}, as the magnitudes of these parameters are now well-established.

For parameters with existing asymmetric constraints $\beta^{+\sigma_{+}}_{-\sigma_{-}}$, this analysis uses an asymmetric Gaussian prior, equivalent to the logarithm of the prior being an asymmetric quadratic:
\begin{equation}
    \ln{P\left(\mu\right)} = \mathcal{A} -
    \begin{cases}
        \frac{\left(\mu-\beta\right)^{2}}{2\sigma_{+}^{2}} & \textrm{if } \mu\geq\beta,\\
        \frac{\left(\mu-\beta\right)^{2}}{2\sigma_{-}^{2}} & \textrm{if } \mu<\beta.
    \end{cases}
\end{equation}
Here, $\mathcal{A}$ is the logarithm of the prior's normalisation constant, and cancels out in the detailed balance condition. For parameters with symmetric constraints, $\sigma_{+}=\sigma_{-}$, then $\ln{P\left(\mu\right)}$ reduces to a quadratic with maximum at $\mu = \beta$.

Fit parameters often also have basic physical limits on what values they can hold. Normalisation parameters, for example, must be positive. The oscillation parameters are restricted to be in the very broad range $\SI{3.0e-6}{\eV\squared}\leq\dmsq{}\leq\SI{1.0e-3}{\eV\squared}$ and $\ang{5}\leq\tonetwo{}\leq\ang{65}$. The prior density beyond these limits is zero, meaning that any proposed step outside the allowed region can be immediately rejected.

\subsection{Including Systematics in the Fit}\label{sec:sys_in_fit_theory}
One important implementation detail is how systematics are applied within the MCMC fitting process. Once systematics are added to the fit, at every step the binned PDFs for all the processes considered in the fit must get modified appropriately, which can become extremely computationally-intensive if not approached carefully. The strategy used in the \texttt{OXO} framework starts by thinking of the contents of a binned PDF as a vector of bin probabilities, $\bm{\mathrm{p}} = \left(p_{1}, p_{2}, ..., p_{N_{\textrm{bins}}}\right)^{T}$. Then, a systematic acting on the PDF can be thought of as a linear transformation, and hence a matrix $S$ acting on this vector: $\bm{\mathrm{p}'} = S\bm{\mathrm{p}}$. This matrix only needs to be calculated once for a given set of systematic parameter values, and can then use the same matrix on all the PDFs in the fit. Furthermore, when multiple systematics are applied, the matrix for each systematic can then be combined via matrix multiplication into one single ``detector response'' matrix. \texttt{OXO} uses the \texttt{Armadillo}~\cite{sandersonArmadilloTemplatebasedLibrary2016,sandersonUserFriendlyHybridSparse2018} % Armadillo software citation
linear algebra package for efficient matrix manipulation.

There is a problem that can arise when considering the impact of systematics near the edge of the analysis ROI. Many systematics such as shifts, scalings, and convolutions use information about the contents of nearby bins to determine the contents of a particular bin. However, for bins near the edge some of that information does not exist --- it has been lost to the cuts that define the ROI. This can lead to a bias in the generation of the modified PDFs, and therefore also the posterior distribution.

This is exemplified by the impact of an energy scale systematic on the energy distribution of PMT $\beta-\gamma$ events in the detector, shown in Fig.~\ref{fig:energy_scaling_example}. Because the events seen for this process in the ROI are merely the high-energy tail, any systematic energy scaling $E'_{\textrm{reco}} := \alpha E_{\textrm{reco}}$ should have a large impact on the number of events observed at the low end of the ROI. However, given that the information about data below $E_{\textrm{min}}$ is lost to the ROI cuts, any energy scaling of $\alpha>1$ will not be applied correctly at all.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{6_SolarAnalysis/images/buffer_region_demonstration_PMT_beta_gammas.pdf}
    \caption[Demonstration of how to handle an energy scaling systematic correctly]
    {Demonstration of how to handle an energy scaling systematic correctly. The distribution of PMT $\beta-\gamma$ events in the analysis ROI is shown versus energy, both before and after an energy scaling of $\alpha = 1.01$ has been applied. If the distribution below \SI{2.5}{\MeV} is not known, then the scaled distribution is biased systematically. This is solved through the use of a buffer region, as indicated.}
    \label{fig:energy_scaling_example}
\end{figure}

The solution to this problem is defining a ``buffer region'' of bins on the edges of the ROI, which allow for tracking of events in and out of the ROI due to systematics, but are not considered when calculating the likelihood. This is also shown in Fig.~\ref{fig:energy_scaling_example}. After the scaling systematic is applied, although incorrect bin values are found in the buffer region, this is fine because the likelihood is no longer being calculated with those bins. Note that because of this modification, the normalisation parameters put into the model no longer represent the expected number of events in the ROI. Instead, they represent the number of events expected in both the ROI and buffer region, before any systematics have been applied.


\subsection{Including Oscillations in the Fit}\label{sec:osc_in_fit}
Within the analysis MCMC code, the process of neutrino oscillations are thought of as a \textit{de facto} systematic that acts only on the \beight{} $\nu_{e}$ and $\nu_{x}$ signal spectra. Three parameters relevant to the signal are floated within the MCMC fit: \dmsq{}, \tonetwo{}, and $\varphi_{\beight{}}=\Phi_{\ce{^{8}B}}^{\mathrm{meas}}/\Phi_{\ce{^{8}B}}$, the unoscillated \ce{^{8}B} neutrino flux relative to the expected rate. For the two signal PDFs, a third ``bookkeeping'' dimension is added on top of reconstructed energy and radius: the true neutrino energy, $E_{\nu}$. This is necessary to correctly apply oscillations, as the oscillation probability is a function of the neutrino's true energy, not the measured energy. Before the fit, these 3D PDFs are given normalisations corresponding to the expectation of the number of events for each type, $\nu_{e}$ and $\nu_{\mu,\tau}$, after cuts but before oscillations have been applied. Strictly speaking there should be zero $\nu_{\mu,\tau}$ events before neutrino oscillations: the pre-oscillation rate used here is the post-cut number of events expected if 100\% of the neutrinos oscillated to the $\nu_{\mu,\tau}$ type.

During the MCMC fit, for a given set of parameters $\bm{\theta} = \left(\dmsq{}, \tonetwo{}, \varphi_{\beight{}}\right)$ the following is performed to oscillate the signal PDFs. Firstly, the normalisations are scaled by the factor $\varphi_{\beight{}}$. Then, for each $E_{\nu}$ bin the survival probability $P_{ee}\left(E_{\nu}, \dmsq{}, \tonetwo{}\right)$ is calculated. Each bin then has their probability scaled by either $P_{ee}$ or $1-P_{ee}$, for $\nu_{e}$ and $\nu_{\mu,\tau}$ respectively. Within the structure of the \texttt{OXO} framework these bin-by-bin scaling are not immediately applied, but instead a matrix describing the impact of oscillations on each of the PDFs is made. Because the oscillation transformation is purely a bin-by-bin scaling, the resulting matrices are diagonal, with diagonal elements $\varphi_{\beight{}}\cdot P_{ee}\left(E_{\nu}, \dmsq{}, \tonetwo{}\right)$ or $\varphi_{\beight{}}\cdot\left(1-P_{ee}\left(E_{\nu}, \dmsq{}, \tonetwo{}\right)\right)$ for $\nu_{e}$ and $\nu_{\mu,\tau}$ respectively. After the oscillation matrix along with all other systematic matrices are applied to the signal PDFs, the PDFs are then marginalised over the $E_{\nu}$ dimension so that the signal PDFs match the dimensionality of all other PDFs.

Calculations of the survival probability are handled with \texttt{PSelmaa}, an algorithm written by Nuno Barros for SNO~\cite{barrosPrecisionMeasurementNeutrino2011}. % Nuno's thesis
This considers not only the neutrino oscillations through the vacuum of space between the Sun and Earth, but also the impact of matter effects in both the Sun and Earth. This can usually be a very computationally-intensive process, but \texttt{PSelmaa} takes advantage of the assumption that the solar oscillation parameters are in the so-called ``Large Mixing Angle'' regime, making the calculation much faster. As seen in Section~\ref{sec:nu_osc_evidence}, % ref chapter 1
previous solar oscillation experiments demonstrate that this assumption is reasonable. For this analysis, the standard MSW effect described in Chapter~\ref{chap:theory} is assumed with neutrinos obeying the Normal Hierarchy, with the Sun following the \texttt{B16\_GS98} metallicity model~\cite{vinyolesB16StandardSolar2018} % CITE
and the \texttt{PREM} model being used for the Earth~\cite{dziewonskiPreliminaryReferenceEarth1981}. % CITE
% Discussion of why these models and not others?

One final thing \texttt{PSelmaa} needs to know to calculate survival probabilities is the distribution of solar zenith angles during the data-taking. The solar zenith $\theta_{z}$ is the angle between the two following vectors: one going from the centre of the Earth through the centre of the SNO+ detector, and another starting from the detector's centre and pointing towards the Sun. As an example, if the Sun were ever to be vertically above the detector, both vectors would be along the vertical direction $\bm{\hat{z}}$ in detector coordinates, leading to a solar zenith angle of $\theta_{z}=0$. The position of the SNO+ detector on Earth, as well as the times at which the detector was live, determine the solar zenith angle distribution. If not accounted for, this can lead to a bias in the result of the analysis, as a preponderance of livetime taken at night would lead to a larger fraction of solar neutrinos having to pass through the bulk of the Earth to get to the detector, and hence the impact of the MSW effect would be greater.

Even after using the Large Mixing Angle approximation, having to call \texttt{PSelmaa} numerous times for every step in the MCMC algorithm would lead to exorbitant run times for the fitting. Therefore, a further approximation is made. Before running the MCMC fit, \texttt{PSelmaa} is used to calculate $P_{ee}$ over the necessary 3D space of parameters. To get a fine scan of this space, 101 $E_{\nu}$ values from \SI{1}{\MeV} to \SI{20}{\MeV}, 101 \dmsq{} values from \SI{3e-6}{\eV\squared} to \SI{1e-3}{\eV\squared}, and 151 values for \tonetwo{} from \ang{5} to \ang{65} were looked over. This 3D grid of $101\cdot101\cdot151$ $P_{ee}$ values is then written to disk, and loaded into memory for use during the fit as a lookup table. At run-time, as the Metropolis-Hastings algorithm samples this 3D space the survival probability is estimated through a trilinear interpolation of the 3D grid loaded in: a version of linear interpolation for three dimensions.
% Discussion of minor systematic associated with linear interpolation?

    %  \begin{itemize}
        % \item Start with observational principle: changes in oscillation parameters lead to change in the energy spectrum of neutrinos elastically-scattering in the detector, and hence a change in the observed energy spectra of elastically-scattered electrons within the detector. Demonstrate basic impact on modifying $\Delta m^{2}_{21}$ and $\theta_{12}$.
        % \item Want to maximise the sensitivity to measuring these two parameters. Energy spectrum of signal is background-free above $\sim\SI{5}{\MeV}$, but rate is substantially larger at lower energies. If one can minimise backgrounds at the lower energies, then there should be hopes of obtaining a measurement with greater precision!
        % \item Set up analysis approach: a Bayesian analysis using MCMC. Explain why this was chosen at a high level first: allows for us to perform a relatively complex analysis with multidimensional PDFs, numerous backgrounds, systematics, constraints on parameters, all whilst allowing us to obtain well-defined measures of uncertainty on our final results.
        % \item Test statistic: binned extended log-likelihood. Explain why this is fundamentally the ``right''  test statistic to use. Allows for handling of constraints and systematics.
        % \item Give an overview of how an MCMC works. Key idea: exploring parameter space in such a way as to reproduce posterior density distribution. Clever! Helps to avoid ``curse of dimensionality'' that often arises in fits with numerous parameters. Must also explain how we work in a fundamentally Bayesian, not frequentist, statistical approach.
        % \item Explain background processes we'll be dealing with: can be distinguished by event topology, energy spectrum, and position distribution.
        % \item Explain cuts that I am using for the analysis. Demonstrate how these attempt to maximise our signal sensitivity. Leads nicely into choice of 2D fit in both energy and radius (cubed).
        % \item Describe implementation of neutrino oscillations within the fitting procedure: flat priors on the oscillation parameters, but a strong constraint from the solar flux. Explain choices of constraint that are possible. MCMC varies oscillation parameters and flux scaling factor, which then modifies the solar signal PDFs through a de-facto systematic that is a function of true neutrino energy, a 3\textsuperscript{rd} ``bookkeeping'' dimension in the signal's PDFs only. Neutrino oscillations simulated via PSelmaa, which accounts for MSW effect in both the Sun and the Earth. For computational speed, at run-time we actually use a lookup table with linear interpolation for the survival probability as a function of parameters.
        % \item Implementation of systematics: we handle them generally as linear transformations acting on the vector of bin data. Clever!
        % \item Which systematics do we expect to be particularly important for this analysis? Well, mismodelling in detector optics etc. can lead to changes in the measured energy spectrum of processes, which can be decomposed into an energy scale term and an energy smearing term to first order. Systematics also possible in the radial dimension (expected to be less important?) 
        
    %  \end{itemize}

    % [19 pages currently, without figures; probably +5 pages for figures.]
\section{Analysis on Scintillator-Phase data}\label{sec:solar_analysis_results}
\subsection{Dataset and Livetime}\label{sec:solar_dataset_livetime}
The data used in this analysis was scintillator phase data after the end of the PPO top-up campaign that completed in April 2022. An initial validation of the analysis tools was performed on data between 29\textsuperscript{th} April and 10\textsuperscript{th} May 2022~\cite{cookmanSolarOscillationAnalysis2023}. % cite presentation of my high-background results
For the full analysis, data was chosen from runs taken after these dates. Not all data taken during this time was considered usable for this analysis, however. The Collaboration's `Preliminary Scintillator Gold' run selection list was used as the basis for this analysis. This run list requires:
\begin{itemize}
    \item The run type must be in `Physics', as opposed to running in calibration or maintenance;
    \item The run must last at least 30 minutes;
    \item Detector electronics must be working in a stable manner without any alarms, and with all crates online;
    \item There are no abnormal rates of tagged muons, and the OWL PMTs are correctly functioning;
    \item There are no unusual conditions from e.g. earthquakes, blasting activity in the mine, or loss of power. 
\end{itemize}
In the end, data for this analysis used `Gold' runs selected between 17\textsuperscript{th} May--30\textsuperscript{th} November 2022, run numbers 300733--306498. The total livetime associated with this dataset was calculated by looking at the start and end times of each run using the detector's \SI{10}{\MHz} clock: 84.977 days.

The livetime of the data actually used in the analysis ends up being somewhat less than this raw value, because of the muon tagger and the high-\texttt{nhit} event tagger described in Section~\ref{sec:event_selection}. This is because these tagging algorithms veto events for a time period following a tagged event. Any events in such a time window are automatically thrown out of consideration for analysis. An algorithm was written to determine the loss of livetime from both of these tagging processes for all the runs selected in this analysis, allowing for the net livetime to be calculated. To ensure accuracy in the value of this lost livetime, the algorithm took care to handle any overlaps in the time veto windows. This ends up being a quite common occurrence, as tagged muon events and their followers often have a very high \texttt{nhit}. The net livetime was calculated to be 80.615 days. Data processing and simulations used RAT versions 7.0.8 and 7.0.9.

As discussed in Section~\ref{sec:osc_in_fit}, in order to account for the impact of the MSW effect through the Earth on $P_{ee}$, the solar zenith angular distribution of the dataset is needed. This was achieved by taking the recorded trigger time of each event within the dataset as given by the GPS-calibrated \SI{10}{\MHz} clock, determining the position of the Sun at that time, and then deriving the value of $\cos{\theta_{z}}$ given that solar position. The resulting distribution of $\cos{\theta_{z}}$ is shown in Fig.~\ref{fig:solar_zenith_distribution}. Also shown in this plot is the zenith distribution expected if data was taken uniformly over the time period between 17\textsuperscript{th} May--30\textsuperscript{th} November 2022. While both distributions have peaks at $\cos{\theta_{z}} =$ -0.3, 0.4, and 0.9, the data distribution has a far more complex structure due to the times of day and year of the runs selected for this analysis.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/solar_zenith_dist_versus_uniform.pdf}
    \caption[Distribution of $\cos{\theta_{z}}$, for the events within this dataset, compared to a uniform time distribution]
    {Distribution of the cosine of the solar zenith angle, $\cos{\theta_{z}}$, for the events within this dataset. If an event has $\cos{\theta_{z}}>0$, then at the time of that event the position of the Sun is above the detector's equator. Also shown is the zenith distribution expected if data was taken uniformly in the period 17\textsuperscript{th} May--30\textsuperscript{th} November 2022.}
    \label{fig:solar_zenith_distribution}
\end{figure}

    % \begin{itemize}
    %     \item Description of dataset chosen for analysis: 2.2 g/L scintillator-phase data that satisfies the ``gold'' list of run selection requirements, between May and November 2022.
    %     \item Explain requirements for run being selected into the Gold list.
    %     \item Note `raw' livetime calculated for this dataset, and then calculate the impact that the muon and high-nhit vetos have on the livetime.
    %     \item Note which RAT versions MC production is being used to compare to data.
    % \end{itemize}
    % [2 pages]
\subsection{Event Selection}\label{sec:event_selection}
Once the runs of processed data and matching MC have been selected, the next step was to perform cuts on both in order to obtain the analysis ROI. The full list of cuts used are shown in Table~\ref{tab:ev_selection}, most of which have already been explained in Section~\ref{sec:background_processes}. The energy and position cuts are contingent on a valid reconstruction having been performed on a given event; however, the \texttt{ScintFitter} is only run on events with a minimum number of hits, and even then the results may not be valid. Therefore, two important cuts in this analysis are to confirm that the \texttt{ScintFitter} was actually run, and that the results are valid.

\begin{table}
    \centering
    \begin{tabular}{c >{\centering\arraybackslash}m{7.5 cm}}
        \hline
        Cut Description                        & Cut                      \\ \hline \hline
        Event triggers detector                & \texttt{evIndex} $\ge 0$ \\
        Pass data cleaning                     & $\mathtt{ANALYSIS\_MASK} = \mathtt{0x2100000042c2}$ \\
        High-\texttt{nhit} veto                & \texttt{correctedNhits} $\ge 5000$; veto for \SI{20}{s} \\
        \texttt{ScintFitter} used              & $\mathtt{scintFit} = \mathrm{true}$ \\
        Results of \texttt{ScintFitter} valid  & $\mathtt{fitValid} = \mathrm{true}$ \\
        In energy Region of Interest           & $\SI{2.5}{\MeV} < E < \SI{14.0}{\MeV}$ \\
        Fiducial Volume cut                    & $R < \SI{5.0}{\metre}$ \\
        Remove BiPo Out-of-window tags         & See Table~\ref{tab:bipo_oow_tagging_requirements} \\
        Remove BiPo In-window tags             & Tagged if $\mathtt{alphaBeta212} > 0$ and $\mathtt{alphaBeta214} > 0$ \\
        Remove External tags                   & Tagged if $E < \SI{5.0}{\MeV}$, $\mathtt{ext0NuTimeTl208AVNaive} > -0.007$, and $\mathtt{ext0NuAngleTl208AV} > -4.7$ \\
        \hline
    \end{tabular}
    \caption{Cuts used in this analysis.}
    \label{tab:ev_selection}
\end{table}

Most of the other cuts used in this analysis that have not yet been discussed come from handling differences between data and MC. In MC, it is possible to simulate an event but not have it trigger the detector. In this case, the stored index of that event (\texttt{evIndex}) will be less than zero. These `events' are never seen in data, but they do need to be removed from simulation. On the flip side, there are types of events that are observed in the detector but do not get simulated. Many of these events are caught during `data cleaning', which looks for a wide variety of problems in the data when the data is being processed. Any subset of the checks made during data cleaning can be chosen to consider in a given analysis by defining a specific `analysis mask'; a number that specifies for each of its binary digits whether to consider the data cleaning process associated with that bit. The mask used in this analysis is the Collaboration's current standard for the scintillator phase, \texttt{0x2100000042c2}, which corresponds to:

\begin{itemize}
    \item \textbf{00 cut}: Tags every event which has a GTID ending in 00 in hexadecimal, which is associated with a long-standing issue in the detector triggering system that occurs when the GTID rolls over.
    \item \textbf{Junk cut}: Tags events that have a channel with multiple recorded hits, which is not possible for a normal event. These are associated with so-called `orphaned' hits that the event building process does not know what to do with.
    \item \textbf{OWL cut}: Tags events that have at least three OWL PMTs hit, a sign that the event came from a cosmic-ray muon.
    \item \textbf{Polling cut}: Tags all events that occur during `polling', a detector monitoring process.
    \item \textbf{CAEN cut}: Tags all events that have missing CAEN trace data.
    \item \textbf{Muon cut}: Tags muon-like events.
    \item \textbf{Muon follower cut}: Tags all events coming up to \SI{20}{\second} after a tagged muon event.
\end{itemize}

Beyond the formal data cleaning cuts, a high-\texttt{nhit} veto was used to tag and remove various instrumental backgrounds which can generate large amounts of light in the detector. This method also catches any muons left untagged by the data cleaning mask. After any event with a sufficiently large number of hits, both that event and any following it for \SI{20}{\second} are removed. This method is likely to be fairly conservative, and there exist different methods for cutting high-\texttt{nhit} instrumental backgrounds within data cleaning. However, these methods were originally coordinated for the water phase, and had not been updated for the scintillator phase at the time of writing.

The results of all these cuts can be seen for the data in Table~\ref{tab:data_cut_effs}. 654 out of the \num{18112495770} events within the dataset pass all cuts. These same cuts were also run over the MC, with results summarised in Table~\ref{tab:MC_cut_effs}. Important to note was the difference between the number of physics events simulated for a given process and the number of triggered events. The latter ends up being larger than the former for all processes discussed here, because a given physics event in the detector will often generate a retrigger event following the primary triggered event. Also shown in this table are the combined cut efficiencies for each process. This efficiency is defined as the number of triggered events that pass all cuts, divided by the number of simulated physics events for that process. It is this value that can be used to convert between the number of physics events expected in a given time period, and the number of observed triggered events observed.

\begin{table}
    \centering
    \begin{tabular}{c r r}
        \hline
        Cut                         & \# Events Remaining & Cut Efficiency (\%) \\ \hline \hline
        Before cuts                 & \num{18112495770} & 100.00   \\
        Event triggers detector     & \num{18112495770} & 100.00   \\
        Data Cleaning               & \num{17663711108} & 97.52    \\
        high-\texttt{nhit} veto     & \num{17039514332} & 96.47    \\
        \texttt{ScintFitter} used   & \num{10433875757} & 61.23    \\
        \texttt{ScintFitter} results valid & \num{3431328125} & 32.89 \\
        Energy cut                  & \num{132673}      & 0.00387 \\
        FV cut                      & \num{2093}        & 1.58    \\
        BiPo OOW cut                & \num{817}         & 39.03   \\
        BiPo IW cut                 & \num{719}         & 88.00   \\
        Externals cut               & \num{652}         & 90.68   \\
        \hline
    \end{tabular}
    \caption{Impact of each cut on the quantity of events in data. The efficiency for a given cut is defined here as the fraction of events which have survived all previous cuts, which also survive that cut.}
    \label{tab:data_cut_effs}
\end{table}

\begin{table}
    \begin{center}
        \begin{tabulary}{\textwidth}{c R R R R}
            \hline
            MC process       & \# Physics Events Simulated & \# Triggered Events Simulated  & \# Remaining after cuts & Overall Cut Efficiency (\%) \\ \hline \hline
            \beight{} $\nu_{e}$, unoscillated & \num{2830425}  & \num{5511568}  & \num{929551} & 32.8     \\
            \beight{} $\nu_{\mu,\tau}$, unoscillated & \num{1898088} & \num{3675261} & \num{572949} & 30.2\\
            \hline
            AV \ce{^{214}Bi}    & \num{27138894} & \num{38267431} & \num{127}    & 0.000468 \\
            Ropes \ce{^{214}Bi} & \num{67541698} & \num{84734663} & \num{221}    & 0.000327 \\
            AV \ce{^{208}Tl}    & \num{4071850}  & \num{6689331}  & \num{1471}   & 0.0361    \\
            Ropes \ce{^{208}Tl} & \num{6105956}  & \num{8758457}  & \num{2407}   & 0.0394   \\
            External Water \ce{^{214}Bi} & \num{81300835} & \num{85056233} & \num{62} & 0.0000763 \\
            External Water \ce{^{208}Tl} & \num{36602319} & \num{38963663} & \num{2786} & 0.00761 \\
            PMT $\beta-\gamma$  & \num{4069055}  & \num{6615601}  & \num{13570}  & 0.333    \\
            \hline
            Internal \ce{^{212}BiPo} & \num{9761554} & \num{24331685} & \num{8276} & 0.0848  \\
            Internal \ce{^{208}Tl} & \num{816233} & \num{1626536} & \num{409629} & 50.2     \\
            Internal \ce{^{214}BiPo} & \num{4883417} & \num{19293285} & \num{1030} & 0.0211 \\
            Internal \ce{^{210}Tl} & \num{815256} & \num{1625012} & \num{342415} & 42.0     \\ \hline
            Surface \alphan{} (combined) & \num{22627103} & \num{48834413} & \num{2238} & 0.00458 \\
            Internal \alphan{} & \num{1219279} & \num{4740313} & \num{655} & 0.0537 \\
            % Cosmogenic \ce{^{11}C} & \num{8139189} & \num{16206469} & \num{0} & 0 \\
            Internal \ce{^{228}Ac} & \num{9764480} & \num{19294643} & \num{0} & 0 \\
            Internal \ce{^{234m}Pa} & \num{16959995} & \num{30806833} & \num{1} & 0.0000059 \\
            \hline
        \end{tabulary}
    \end{center}
    \caption[Combined impact of cuts on each MC process]
    {Combined impact of cuts on each MC process. Overall cut efficiency is defined as the number of remaining triggered events after all cuts have been applied, divided by the number of physics events simulated for that process. Processes have been split into three general categories: (unoscillated) signal, external backgrounds, and internal backgrounds. A final category corresponds to other processes that were considered for this analysis, but either all events are cut out, or Section~\ref{sec:exp_rates_constraints} will show a negligible number of these events are expected in our dataset.}
    \label{tab:MC_cut_effs}
\end{table}

Variable bin widths in energy were chosen for this analysis: \SI{0.1}{\MeV} bins between \SI{2.5}{\MeV}--\SI{5.0}{\MeV}, \SI{0.25}{\MeV} bins between \SI{5.0}{\MeV}--\SI{13.0}{\MeV}, and a single energy bin between \SI{13.0}{\MeV}--\SI{14.0}{\MeV}. Four equally-spaced bins were used in the parameter $r_{3} = (r_{\mathrm{reco}}/R_{\mathrm{AV}})^{3}$ in the range $\SI{0}{\m}\le r_{\mathrm{reco}} < \SI{5.0}{\m}$. The $r_{3}$ parameter was used to allow for equal volume weighting for each radial bin. This binning for energy and $r_{3}$ was chosen as a balance between wanting to get as much information about the distributions of each process as possible, whilst ensuring that there were sufficient statistics for the MC PDFs in every bin.


    % \begin{itemize}
    %     \item List final set of cuts chosen for analysis, along with any explanations for cuts that haven't already been motivated earlier (e.g. data cleaning). These are:
    %     \begin{itemize}
    %         \item Event index cut (prevents MC events that don't trigger detector from being used)
    %         \item Data cleaning cuts
    %         \item High-nhit event timing veto cut
    %         \item Valid scintFit reconstruction
    %         \item Reconstructed energy $2.5 < E < 14$
    %         \item Radial fiducial volume cut
    %         \item BiPo out-of-window tag
    %         \item BiPo in-window classifier cut
    %         \item Externals classifier cut
    %         \item ``Cleanliness'' cut
    %         \item Position FOM cut
    %     \end{itemize}
    %     \item Show impact of cuts on data and MC. Show tables (the full details maybe in an appendix) indicating this.
    %     \item Describe the finalised choice of binning for PDFs and data.
    % \end{itemize}
    % [4 pages]
\subsection{Expected Rates and their Constraints}\label{sec:exp_rates_constraints}
Because this analysis is dependent on deriving the shape and normalisation of the \beight{} spectrum from the data observed, it is important to know the number of events expected to be seen within the dataset for each process, both signal and background. This is done in two stages: firstly, the expected rates for each process before any cuts are applied is determined, along with any constraints. Then, by using the cut efficiencies calculated in the previous section, estimates for the expected number of events after cuts can be derived. When combined with a constraining uncertainty, this can be used as a prior within the MCMC fit. The resulting expected rates before and after cuts, along with any constraints being used, are shown in Table~\ref{tab:MC_expected_rates_constraints}. Details of how the rates and constraints were calculated can be found in Appendix~\ref{chap:appendix_solar_rates}.

\begin{table}[!th]
    \begin{center}
        \begin{tabulary}{\textwidth}{c R R R R}
            \hline
            MC process       & Expected \# Events Pre-Cuts & Overall Cut Efficiency (\%) & Expected \# Events Post-Cuts & Constraint (\%) \\ \hline \hline
            \beight{} $\nu_{e}$, unoscillated & 572.58 & 32.8       & 187.94 & $^{+2.5\%}_{-1.7\%}$ \\
            \beight{} $\nu_{\mu,\tau}$, unoscillated & 102.22 & 30.2       & 30.86  & $^{+2.5\%}_{-1.7\%}$ \\
            \hline
            AV \ce{^{214}Bi}             &\num{594000}&0.000468& 2.78   & $^{+304.8\%}_{-100\%}$ \\
            Ropes \ce{^{214}Bi}          &\num{140000}&0.000327& 0.46   & $^{+304.8\%}_{-100\%}$ \\
            AV \ce{^{208}Tl}             &\num{69600} & 0.0361 & 25.14  & $^{+304.8\%}_{-100\%}$ \\
            Ropes \ce{^{208}Tl}          &\num{79900} & 0.0394 & 31.48  & $^{+304.8\%}_{-100\%}$ \\
            External Water \ce{^{214}Bi} &\num{4490000}&0.0000763& 3.42 & --- \\
            External Water \ce{^{208}Tl} &\num{190000}& 0.00761&  14.46 & --- \\
            PMT $\beta-\gamma$           &\num{16800} & 0.333  &  56.12 & $^{+111.5\%}_{-40.5\%}$ \\
            \hline
            Internal \ce{^{212}Bi-Po}     & \num{1814} & 0.0848 & 1.54   & $\pm 25\%$  \\
            Internal \ce{^{208}Tl}       & \num{1021} & 50.2   & 512.39 & --- \\
            Internal \ce{^{214}Bi-Po}     & \num{11728}& 0.0211 & 2.47   & $\pm 25\%$ \\
            Internal \ce{^{210}Tl}       & \num{2.5}  & 42.0   & 1.05   & $\pm 25\%$ \\ \hline
            Surface \alphan{} (combined) & \num{554.4}& 0.00458& 0.07   & --- \\
            Internal \alphan{}           & \num{17.0} & 0.0537 & 0.009  & --- \\
            % Cosmogenic \ce{^{11}C}       & \num{196.2}& 0      & 0      & --- \\
            Internal \ce{^{228}Ac}       & \num{2835} & 0      & 0      & --- \\
            Internal \ce{^{234m}Pa}      & \num{11726}&0.0000059& 0.0007& --- \\
            \hline
        \end{tabulary}
    \end{center}
    \caption[Number of events expected both before and after cuts, along with any constraints]
    {Number of events expected both before and after cuts, along with any constraints. As in Table~\ref{tab:MC_cut_effs}, processes have been split into broad categories, with the last one being processes which have negligible rates after cuts. These processes are not included within the analysis fit.}
    \label{tab:MC_expected_rates_constraints}
\end{table}


% \subsubsection{Cosmogenic Backgrounds}
% The final background in consideration is decays of the cosmogenically-induced isotope \ce{^{11}C}. Extrapolating from the KamLAND experiment, we expect before cuts 889 of these events a year~\cite{}. % cite KamLAND, someone in SNO+ who's done this?
% However, the cut efficiency for this background is zero, 

\subsection{Systematics}
There are a number of systematic effects in the analysis that could possibly have some impact on the resulting posterior densities of \tonetwo{} and \dmsq{}. Consideration of these effects is important to ensure that the uncertainty in the measurement is not underestimated. However, Section~\ref{sec:solar_projections} will show that the analysis is statistically-limited for this dataset, and so it is not necessary to perform exhaustive measurements to determine the contribution of all possible systematic contributions. Instead, the focus shall be on the subset of the systematics that can plausibly have the most impact on the final measurement.

In this analysis, the measurement of the oscillation parameters is based on the fitted shape and normalisation of the \beight{} signal energy spectrum. Therefore, for a systematic effect to have an impact on the final measurement it must first impact the fit of the signal PDF. One straightforward way this could occur is for some background events to be misattributed as signal events within the fit (or vice versa). If the rates of signal and background processes are strongly constrained to incorrect values, then this could easily happen. This could be a result of either getting the rate pre-cuts or the cut efficiencies incorrect. In this analysis, the strongest constraint on the rate of events comes from the global fit constraint of $\Phi_{\beight{}}$. Because of this, the fit will also be run with the looser SSM constraint on the signal flux as a means of comparison.

Another class of systematic effects that can plausibly have a substantial impact come from the mismodelling of the detector response, particularly on the reconstructed energy and radius, as they are the observables used within the fit. The most important of these for this analysis is a global mismodelling of the energy calibration by some linear factor known as the energy scale, $\alpha$: $E_{\mathrm{reco}}\to\alpha E_{\mathrm{reco}}$.

Because of this importance, it is worthwhile constraining this $\alpha$ parameter. This can be done by comparing the reconstructed energy distributions of \ce{^{214}BiPo} events tagged by R. Hunt-Stokes~\cite{hunt-stokesPrivateCommunication2023} % cite Rafa?
in the dataset to the equivalent production MC, in the same \SI{5}{\m} FV. For a given $\alpha$, the energies of the tagged \ce{^{214}Bi} in MC were scaled by $\alpha$, and then compared to the equivalent (unscaled) distribution in data, via a log-likelihood ratio $\mathcal{L}_{\mathrm{Bi}}$:
\begin{equation}
    \mathcal{L}_{\mathrm{Bi}} = 
    \sum_{i=1}^{N_{\mathrm{bins}}}
        n_{i}^{\mathrm{data}}\ln{\frac{n_{i}^{\mathrm{MC}}}{N_{\mathrm{MC}}}}.
\end{equation}
Here, $n_{i}^{\mathrm{data}}$ and $n_{i}^{\mathrm{MC}}$ are the number of events for data and (scaled) MC in energy bin $i$, with $N_{\mathrm{bins}}$ being the total number of bins and $N_{\mathrm{MC}}$ being the total number of tagged events in MC. To account for the edge effects discussed in Section~\ref{sec:sys_in_fit_theory}, the first and last bins in energy were used as buffers and not considered in calculation of $\mathcal{L}_{\mathrm{Bi}}$.

Values of $\alpha$ were scanned over, generating a log-likelihood distribution. In the usual way, a constraint on $\alpha$ was obtained by obtaining the energy scaling factor which maximises the log-likelihood, followed by looking for the $\alpha$ values with values of $\mathcal{L}_{\mathrm{Bi}}$ less than the maximum by 1/2. The constraint was found to be $\alpha = 0.9969\pm0.0022$, with the data and scaled MC at the best fit value of $\alpha$ shown in Fig.~\ref{fig:bi214_escale_calibration}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/rafael_tagged_bi214_data_vs_mc_energy_comparison_resized.pdf}
    \caption[Comparison of tagged \ce{^{214}Bi} reconstructed energy distributions between data and MC]
    {Comparison of tagged \ce{^{214}Bi} reconstructed energy distributions between data and MC, after MC has been scaled by the best-fit factor $\alpha = 0.9969$.}
    \label{fig:bi214_escale_calibration}
\end{figure}

To handle energy scaling as a systematic in this analysis, the energy scale is floated within the MCMC fit, with the constraint given above. The first and last energy bins of the MCMC fit, \SIrange{2.5}{2.6}{\MeV} and \SIrange{13}{14}{\MeV}, were used as buffer bins to let this floating occur.
The energy scale was then allowed to float in the range $\frac{13}{14}\leq\alpha\leq\frac{2.6}{2.5}$, which ensures that scaling could never go beyond the maximum or minimum allowed by the size of the buffer bins. This allowed region is much larger than the constraint found above.

In addition to a global energy scaling, it is possible that the resolution of the energy reconstruction could be systematically off. Because of the effects discussed in Section~\ref{sec:solar_obs_principle}, changes to the oscillation parameters only impact the observed signal distribution over a broad range of energies. This means that mismodelling the energy resolution of the detector is only liable to have an impact on the measurement if that mismodelling is substantial. However, by looking again at Fig.~\ref{fig:bi214_escale_calibration} there appears to be a good qualitative match of the energy resolution between data and MC for the tagged \ce{^{214}Bi} events. As a result, the systematic effect of additional energy smearing will not be considered in this thesis.

Two other possible energy systematics could also exist. There could be an offset in the reconstructed energy, and even some degree of non-linearity. This would lead to a mismodelling of the form $E_{\mathrm{reco}}\to E_{0} + \alpha E_{\mathrm{reco}} +\beta E_{\mathrm{reco}}^{2}$, where $E_{0}$ and $\beta$ are the energy offset and non-linearity systematic parameters. Calibrating all of these systematics together would require at least three calibration sources. This has not been done in this thesis, and so for now these two systematics have been ignored.

Similar to the systematics in reconstructed energy, there are also systematics possible in reconstructed position. These can be decomposed into three kinds of systematic: radial scaling, radial resolution, and position bias along each coordinate axis. The dominant effect that these systematics will have on the PDFs is to change their normalisations. Because all normalisations are being floated within the MCMC fit, and the normalisation constraints of almost all PDFs are relatively large, the impact of these systematics is expected to be sub-dominant.

Finally, there can also be mismodelling of the radial distributions for given processes in MC compared to data. For example, it is known that the internal \ce{^{208}Tl} events are non-uniform as a function of radius; this is why the normalisation of each radial slice for this background is floated independently. The external backgrounds can also plausibly have PDF shape systematics, especially if the assumptions made to model them (such as the use of simulation shells) are not fully accurate. These shape systematics can also arise from the lack of statistics in the PDFs used to model them: as an example, Table~\ref{tab:MC_cut_effs} shows that only 62 external water \ce{^{214}Bi} events are used in the creation of the associated PDF. Handling this type of modelling uncertainties for the external backgrounds is not considered in this thesis.

% \begin{itemize}
%     \item Incorrect constraints of signal and background rates, which could arise from a systematic error in either the expected rate before cuts, or the overall cut efficiency.
%     \item Systematic errors in the reconstructed energy. To first order, this can be decomposed into two parts. Firstly, there could be a linear scaling of the energy by some energy scaling factor $\alpha$, i.e. $E_{\mathrm{reco}}\to\alpha E_{\mathrm{reco}}$. There could also be a difference in the resolution of the energy reconstruction.
%     \item Systematic errors in the reconstructed position. Similar to the above, this can also be decomposed into a radial scaling systematic, radial resolution systematic, and biases along each coordinate axis.
%     \item Mismodelling of the radial distributions of events in MC compared to data.
% \end{itemize}


% \begin{itemize}
%     \item Show expected rates calculation for both signal and background.
%     \item Describe the constraints chosen to apply to the fit, and why they can be justified. These are: 
%     \begin{itemize}
%         \item B8 flux constraint
%         \item U-238 and Th-232 constraints from BiPo tagging
%         \item Alpha-n constraint from Po-210 tagging by Serena and Shengzhao
%         \item External constraints from Tony Zummo's water-phase externals analysis.
%     \end{itemize}
%     \item Describe the systematics to be added to the fit (just energy scale for now, maybe also energy smearing?). For other possible systematics, such as those in position, my aim is to explain why they are sub-dominant and so don't need to be added to the fit. Will cover more about impact of systematics in the next subsection.
% \end{itemize}
% [4 pages]

\subsection{Results}
\subsubsection{Fit Validation}
In theory, the convergence of the sampled distribution within an MCMC fit to the posterior density distribution is guaranteed to eventually occur, regardless of the specific shape of the multivariate Gaussian proposal distribution. However, the speed of this convergence is highly dependent on the widths of the proposal distribution. This is very important as in practice the MCMC fit can only be run for a finite amount of time. For a given parameter within the fit, if the associated width (also known as the step size) in the proposal distribution is too large, then the vast majority of proposals will be of positions in the parameter space where the log-likelihood is less than before, and so these proposals will be consistently rejected. Alternatively,  if the width is chosen too small, the acceptance rate of the chain will be very high, but too much time will be spent for the chain to simply explore the space.

By choosing sensible width parameters for the proposal distribution, as well as running the chain for as long as possible, one can attempt to maximise the ``effective sample size'' of the MCMC fit. The effective sample size is the theoretical number of independent samples that would need to be drawn from the posterior density distribution to have equivalent statistics to that of the MCMC sampling process. It has been shown~\cite{gelmanChapter11Basics2013} that, for a wide variety of situations, an appropriate size for the width of a given parameter is the standard deviation of the true posterior density distribution. % cite appropriate
Because knowing this posterior density distribution is precisely the aim of the whole MCMC procedure, this can make choosing these widths challenging in practice. In this analysis, an iterative approach was used: an MCMC fit was first run using width parameters that were guessed. Once this fit completed, the resulting sampled distribution marginalised onto each fit parameter were looked at to find approximately the magnitude of their standard deviations. This then informed the widths chosen for the next MCMC fit. 
This process continued until two conditions were met simultaneously:
\begin{itemize}
    \item The sampled distribution marginalised over every fit parameter appeared smooth;
    \item The marginalised distributions also appeared to fully explore their parameter spaces many times.
\end{itemize}
% This process was repeated until the autocorrelation of the sampled distribution for each fit parameter appeared to become zero for any step delay that was a substantial fraction of the whole chain length. 
Such a result indicated that the effective sample size has been maximised. A chain length of \num{1000000} steps was found to be sufficient to obtain large enough effective sample sizes for this analysis.

In order to guard against the possibility of a given chain missing some important part of the parameter space because of its starting position, 100 chains were run simultaneously. The initial positions of each in the parameter space were chosen randomly according to the overall prior distribution. Fig.~\ref{fig:params_vs_step_plots} shows the sampled values for four different parameters in a given chain, after the tuning of the width parameters had been completed.

\begin{figure}
    \centering
    \begin{subfigure}{0.98\textwidth}
        \centering
        \includegraphics[width=\textwidth]{6_SolarAnalysis/images/b8_flux_vs_step_MCMC_2.png}
        \caption{$\Phi_{\beight{}}$ (normalised so that the nominal rate equals 1)}
        \label{fig:b8_flux_factor_vs_step}
    \end{subfigure}
    \begin{subfigure}{0.98\textwidth}
        \centering
        \includegraphics[width=\textwidth]{6_SolarAnalysis/images/pmt_betagamma_vs_step_MCMC_2.png}
        \caption{Normalisation of the PMT $\beta-\gamma$ background}
        \label{fig:pmt_vs_step}
    \end{subfigure}
    \begin{subfigure}{0.98\textwidth}
        \centering
        \includegraphics[width=\textwidth]{6_SolarAnalysis/images/dmsq_21_vs_step_MCMC_2.png}
        \caption{\dmsq{}}
        \label{fig:dmsq_21_vs_step}
    \end{subfigure}
    \begin{subfigure}{0.98\textwidth}
        \centering
        \includegraphics[width=\textwidth]{6_SolarAnalysis/images/theta_12_vs_step_MCMC_2.png}
        \caption{\tonetwo{}}
        \label{fig:theta_12_vs_step}
    \end{subfigure}
    \caption[Examples of the sampled values for parameters within a given MCMC chain as a function of the step number]
    {Examples of the sampled values for parameters within a given MCMC chain as a function of the step number, after tuning the proposal distribution width parameters.}
    \label{fig:params_vs_step_plots}
\end{figure}

Given some initial start point in the parameter space, a given chain will typically take some time in finding where the region of greatest likelihood is. This leads to an initial set of steps where the chain moves a large distance in the parameter space in the same direction. 
% , so that the autocorrelation of the chain in this period is substantial for any chosen step delay.
These initial samples are not representative of the actual posterior density distribution, and are called the ``burn-in'' period. Fortunately, enough tuning of the width parameters was performed that there was minimal burn-in observed in any of the chains: this can be observed in Fig.~\ref{fig:params_vs_step_plots}. 
% Fig.~\ref{fig:auto_corr} shows the auto-correlation distributions in a typical chain for a couple of parameters, as well as the log-likelihood. As can be seen, by the time the number of steps differed by \num{1000}, the autocorrelation of parameters and the log-likelihood itself of each chain was at zero, indicating the likelihood evaluations in the chains were independent when they differed by at least \num{1000} steps. 
To be very safe, for all chains the first \num{100000} steps were declared as the burn-in period. The overall set of samples used to approximate the posterior density distribution corresponds to collection of all sampled points on all the chains, after burn-in.

In addition to the above, the MCMC fitting code was also tested on a series of `fake' datasets, including those described in Section~\ref{sec:solar_projections}. These were able to validate that if a `fake' dataset was generated with a given set of parameters, the MCMC fitter would correctly generate posterior densities consistent with those input parameters.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{6_SolarAnalysis/images/autocorr_plot_llh_dmsq_theta12.pdf}
%     \caption[Plot showing the auto-correlation as a function of step delay for three variables in a typical chain]
%     {Plot showing the auto-correlation as a function of step delay for three variables in a typical chain: log-likelihood (blue), \dmsq{} (red), and \tonetwo{} (green).}
%     \label{fig:auto_corr}
% \end{figure}

    % \begin{itemize}
    %     \item Show plots of parameter values versus step, to demonstrate that the step sizes have been tuned sufficiently.
    %     \item Show auto-correlation plots, to motivate a sensible ``burn-in'' size.
    % \end{itemize}
    % [6 pages]

\subsubsection{Oscillation Fit Results}
Now that there is confidence in the convergence of the MCMC fit to the posterior density distribution, the results of this analysis can be obtained. Fig.~\ref{fig:nuisance_params_marged} shows the posterior density distribution, marginalised onto each parameter of the fit other than the two oscillation parameters. Each marginalised distribution appears smooth, another qualitative indication of a sufficient effective sample size in the MCMC fit. For each 1D distribution, the bin centre associated with the HPD has been indicated, and the vertical band indicates the Bayesian 1$\sigma$ CI for that parameter. Also shown in these plots is the prior distribution for each of the parameters, for comparison.

\begin{figure}
    \centering
    \includegraphics[width=0.87\textwidth]{6_SolarAnalysis/images/posterior_densities_plot_no_osc.pdf}
    \caption[Marginalised 1D posterior density distributions for all non-oscillation parameters]
    {Marginalised 1D posterior density distributions for all non-oscillation parameters, shown in blue. The HPD and 1$\sigma$ Bayesian CI are shown for each parameter. Also shown in dashed red are the associated prior distributions.}
    \label{fig:nuisance_params_marged}
\end{figure}

For a number of the parameters, the posterior distributions are more strongly peaked than their associated priors. This indicates that there was enough information in the dataset to further constrain those parameters. One such parameter is the rate of the external water \ce{^{214}Bi}. The resulting measurement of this is $34.1^{+18.5}_{-18.9}$ events in the dataset. This is 10 times larger than the expected rate shown in Table~\ref{tab:MC_expected_rates_constraints}, providing some evidence for a change in the level of this particular background relative to the water phase. In contrast, all other external backgrounds have HPDs consistent with their prior expected values.

Another set of parameters that have been well-constrained in the fit are the normalisations of each of the internal \ce{^{208}Tl} slices. These slices have been numbered radially, with the innermost radial slice being slice 1. 
As expected, slices corresponding to larger radii have a greater fitted rate, with the number of events measured in the outermost radial slice to be twice that of the innermost one. However, the posteriors of the other internal \ce{^{238}U}- and \ce{^{232}Th}-chain are the same as their priors, indicating there was no information from data within the fit to constrain those processes any further. Similarly, the posteriors of the energy scale parameter and $\Phi_{\beight{}}$ only show very mild differences with their strong priors.

Fig.~\ref{fig:corr_plots_params} shows the correlation coefficients associated with the 2D marginalised distributions, for each pair of parameters. From this, clear anti-correlation between a number of the external backgrounds can be seen. This seems intuitive: the PDFs for many of these external backgrounds are similar, and so the fit struggles to distinguish between them.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/corr_coeff2.pdf}
    \caption[Correlation matrix between all parameters in the MCMC fit]
    {Correlation matrix between all parameters in the MCMC fit.}
    \label{fig:corr_plots_params}
\end{figure}

The only other pair of parameters which have a substantial correlation between one another are the two oscillation parameters. Fig.~\ref{fig:2d_osc_param_posterior} shows their 2D marginalised posterior distribution, along with contours for a number of Bayesian CIs. Because the priors on both oscillation parameters are uniform, this can be directly converted into the log-likelihood ratio $\ln{L_{R}}$ for a given bin, where $L_{R} = p/p_{max}$: $p$ is the posterior density of a given bin in the 2D space, and $p_{max}$ is the posterior density of the HPD bin, equivalent to the bin with the maximum likelihood. This allows for Frequentist CI to be derived as well. For this plot, the HPD is at $\tonetwo{} = \ang{34.1}, \dmsq{} = \SI{1.80e-5}{\eV\squared}$.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/steve_style_contours.png}
    \caption[2D posterior density distribution marginalised onto \dmsq{} and \tonetwo{}]
    {2D posterior density distribution marginalised onto \dmsq{} and \tonetwo{}. The colour axis shows $-2\ln{L_{R}}$, proportional to the natural logarithm of the posterior density, with labels corresponding to the values associated with the $N\sigma$ Frequentist CIs. Also shown, in shades of brown, are the Bayesian CIs.}
    \label{fig:2d_osc_param_posterior}
\end{figure}

As can be seen from this plot, the fit has been able to successfully constrain \tonetwo{}. However, beyond a value of \SI{5e-4}{\eV\squared}, the posterior density appears flat as a function of \dmsq{}. This implies that this dataset provides little ability to reject the possibility of large values of \dmsq{}.

Given this lack of constraining power on \dmsq{}, it is reasonable to only obtain a measurement for \tonetwo{}. The result of marginalising over \dmsq{} to obtain the 1D posterior distribution in \tonetwo{} is shown in Fig.~\ref{fig:1d_theta_12_posterior}. From this, we can obtain a measurement of:
\begin{equation*}
    \left|\tonetwo{} + n\cdot180\right|^{\circ} = 38.9^{\circ+8.0^{\circ}}_{\phantom{\circ}-7.9^{\circ}}.
\end{equation*}
Here, the HPD has been estimated by fitting a quadratic function to the peak of the posterior distribution, and the uncertainties by obtaining the Bayesian $1\sigma$ CI. For comparison, the NuFit 5.1 global fit results have~\cite{estebanFateHintsUpdated2020}: % cite NuFit
\begin{equation*}
    \left|\tonetwo{} + n\cdot180\right|^{\circ} = 33.44^{\circ+0.77^{\circ}}_{\phantom{\circ}-0.74^{\circ}}.
\end{equation*}
As can be seen, the measurement made in this thesis is consistent with the global fit value, although with a substantially larger uncertainty, due to the limited statistics of the dataset.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/theta_12_comparison_flux_constraint_corrected_tagging_2_mobins.pdf}
    \caption[Comparison of 1D posterior density distribution marginalised onto \tonetwo{} for different flux constraints]
    {Comparison of 1D posterior density distributions marginalised onto \tonetwo{}, using both forms of \beight{} flux constraint. The 1$\sigma$ Bayesian CI are shown for both.}
    \label{fig:1d_theta_12_posterior}
\end{figure}

Looking back at Fig.~\ref{fig:2d_osc_param_posterior}, the preferred values of \tonetwo{} for large \dmsq{} are somewhat greater on average than for \dmsq{} values close to the global fit value of \SI{7.4e-5}{\eV\squared}. As a result, the final value quoted for \tonetwo{} is dependent on the choice of the prior distribution for \dmsq{}: this is an inevitable result of using a Bayesian framework for this analysis. The impact of choosing a less conservative prior for \dmsq{} can be seen by ignoring all sampled points with $\dmsq{} \geq \SI{5e-4}{\eV\squared}$, as an example. The resulting measurement of \tonetwo{} becomes:
\begin{equation*}
    \left|\tonetwo{} + n\cdot180\right|^{\circ} = 36.4^{\circ+8.0^{\circ}}_{\phantom{\circ}-7.8^{\circ}}.
\end{equation*}
Although there is negligible change to the uncertainty of the measurement, this change in prior shifts the HPD value down by \ang{2.5}, in the direction of the global fit value.

Also shown in Fig.~\ref{fig:2d_osc_param_posterior} is the posterior density for \tonetwo{} if the looser SSM flux constraint described in Appendix~\ref{chap:appendix_solar_rates} is used. Under these conditions, the measurement becomes:
\begin{equation*}
    \left|\tonetwo{} + n\cdot180\right|^{\circ} = 39.0^{\circ+9.8^{\circ}}_{\phantom{\circ}-9.6^{\circ}}.
\end{equation*}
Using this constraint has little impact on the HPD value, but does increase the uncertainty by $\sim\ang{1.8}$. This gives an indication of the extent to which the strong global flux constraint allows \tonetwo{} to be constrained on its own, without any information from the dataset.

One could na\"{i}vely expect that the point in parameter space where each fit parameter corresponds to the HPD value obtained in the above 1D marginalisation would correspond to the maximum likelihood. This is not the case, because of the correlations between parameters in the fit. Because an MCMC fit does not actually attempt to directly find this maximum likelihood point, deciding on the overall `best-fit' point in parameter space is a little ambiguous. For this analysis, the sampled point in all of the chains which had the greatest log-likelihood was declared as the best-fit point. A comparison between the parameter values obtained at the HPD of the 1D marginalisations to the maximum likelihood point is shown in Table~\ref{tab:fit_params_comparison}.

\begin{table}[!th]
    \centering
    \begin{tabular}{c r r}
        \hline
        Fit Parameter & 1D HPD value & Maximum likelihood value \\ \hline \hline
        Relative \beight{} flux & 0.9975 & 1.006  \\
        \dmsq{} [$\times10^{-5}\;\si{\eV\squared}$] & 11.02 & 5.50 \\
        \tonetwo{} [$^{\circ}$]  & 39.05  & 33.73 \\
        \hline
        AV \ce{^{214}Bi} Events     & 3.66 & 8.49 \\
        Ropes \ce{^{214}Bi} Events  & 0.53 & 0.52 \\
        AV \ce{^{208}Tl} Events     & 16.71& 19.08 \\
        Ropes \ce{^{208}Tl} Events  & 27.81& 31.20 \\
        External Water \ce{^{214}Bi} Events & 34.09 & 45.93 \\
        External Water \ce{^{208}Tl} Events & 3.71  & 88.03 \\
        PMT $\beta-\gamma$ Events   & 51.22& 46.88 \\
        \hline
        Internal \ce{^{212}BiPo} Events & 1.54 & 1.45\\
        Internal \ce{^{208}Tl} Events, Slice 1 & 66.52 & 68.78 \\
        Internal \ce{^{208}Tl} Events, Slice 2 & 70.60 & 66.62 \\
        Internal \ce{^{208}Tl} Events, Slice 3 & 87.09 & 87.33 \\
        Internal \ce{^{208}Tl} Events, Slice 4 & 128.92 & 125.25 \\
        Internal \ce{^{214}BiPo} Events & 2.51 & 2.56 \\
        Internal \ce{^{210}Tl} Events & 1.05 & 1.00 \\
        \hline
        Energy Scale Factor, $\alpha$ & 0.9964 & 0.9970\\
        \hline
    \end{tabular}
    \caption[Comparison of the fit parameter values obtained when getting the HPD values after marginalising onto each parameter, versus looking at the sampled point with the maximal likelihood found]
    {Comparison of the fit parameter values obtained when getting the HPD values after marginalising onto each parameter, versus looking at the sampled point with the maximal likelihood found.}
    \label{tab:fit_params_comparison}
\end{table}

Using the maximal likelihood fit parameters, the comparison of data to MC is shown in Fig.~\ref{fig:solar_data_mc_slices_comparison}. The \beight{} signal distribution only dominates above \SI{5}{\MeV}, as well as at low energies for the first two radial slices. This is because for larger radii, external backgrounds become the dominant processes in the \SIrange{2.5}{2.8}{\MeV} region. Between \SIrange{3}{5}{\MeV}, at all radii the internal \ce{^{208}Tl} shows a clear peak well above the \beight{} signal. Qualitatively, the MC appears to give a good fit to the data.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/data_mc_fit_plot_1D_energy_r3_slices_log_maxlh_tannerstyle_2.pdf}
    \caption[Comparison of data to MC for each radial slice]
    {Comparison of data to MC for each radial slice, using the best-fit parameter values derived from the MCMC results.}
    \label{fig:solar_data_mc_slices_comparison}
\end{figure}

% \begin{itemize}
%     \item Posterior density plots for each nuisance parameter, to check that they all look sensible and have sufficient statistics.
%     \item Show plot of correlation coefficients between parameters, and note any strongly-correlated parameters.
%     \item Look at the data versus `best-fit' MC plot in energy, radius, and both. (Recall that in MCMC, the `best-fit' is not the point of parameter scape reached at the end, but the point of highest posterior density). Is there a good fit to data? Any clear disagreements?
%     \item Show 2D contour plot for oscillation parameter posterior density. Note salient features. Show 1D posterior densities for each oscillation parameter. Derive measurement result for $\theta_{12}$, i.e. point of highest posterior density, with uncertainties given by the $1\sigma$ credible interval.
% \end{itemize}
% [5 pages]

% \subsubsection{Impact of Systematics}\label{sec:systematics_impact}

%     \begin{itemize}
%         \item Show impact of modifying certain constraints on the final results of the measurement of $\theta_{12}$. In particular: fiducial volume, \beight{} flux constraint.
%         \item Discussion of systematics post-fit --- Hopefully energy scaling parameter should be close to 1, given the Collaboration's calibration of the optics (light yield in particular). Perform a scan over energy smearing, and check impact of possible radial scale systematic.
%     \end{itemize}
%     [8 pages]
    
\section{Sensitivity Projections}\label{sec:solar_projections}
Given the limited statistics used in the dataset analysed in Section~\ref{sec:solar_analysis_results}, the question of how much better the solar oscillation parameters could be measured with more data in SNO+ naturally arises. In particular, it is worthwhile knowing the expected sensitivity of this analysis as a function of livetime, and hence whether SNO+ could eventually make a world-leading measurement of \tonetwo{} via \beight{} neutrinos. Furthermore, it is useful to find out to what extent improvements such as additional background reduction could help the measurement.

To perform these sensitivity projections, the same analysis method was employed as with the real dataset. However, for each projection scenario a fake ``Asimov'' dataset was generated. This was a 2D histogram binned in the same way as the MC PDFs, corresponding to the total expected rate of all signal and background processes. Because the same PDFs were used to build both the MC and fake dataset, the maximum likelihood should occur when the fit parameters are identical to those which generated the fake dataset. Although this method of performing sensitivity projections leads to the slightly odd situation of having non-integer numbers of events in the bins of the fake dataset, it has been shown that this approach allows us to estimate the median sensitivity of a scenario~\cite{cowanAsymptoticFormulaeLikelihoodbased2011}. % cite https://arxiv.org/pdf/1007.1727.pdf, maybe more?

The assumptions used to make the fake datasets are as follows. To begin with, no cuts were changed between the main analysis and this projection, and the same PDFs were used in both. Furthermore, the \beight{} rate was determined by the same global fit flux value and neutrino oscillation parameters used in the rest of this analysis. Because some of the dataset used in the main analysis was taken during a time of known elevated \ce{^{238}U}- and \ce{^{232}Th}-chain backgrounds, R. Hunt-Stokes provided an additional estimate of the \ce{^{214}Bi-Po} and \ce{^{212}Bi-Po} rates in the \SI{5.0}{\metre} FV for only the period of the dataset where the internal background levels had stabilised. Those rates went from 6.06 events per hour before excluding the higher background period down to 4.87 events per hour for the \ce{^{214}Bi-Po}, and from 0.94 events per hour down to 0.89 events per hour for the \ce{^{212}Bi-Po}. These relative changes for the two background chains were used to scale the rates from their expectation in the original dataset. For the internal \ce{^{208}Tl}, the rate of each slice relative to one another was taken from their fitted HPD values in the original analysis, with the absolute value of those rates scaled by the fractional change in rate between the two time periods, $0.89/0.94$.

External backgrounds were in general set to the rates expected from T. Zummo, as described in Section~\ref{sec:rates_ext_backgrounds}. The exceptions were the backgrounds from the external water: for the \ce{^{208}Tl} component, the target rate of this background as described in~\cite{andringaCurrentStatusFuture2016} was used. For the \ce{^{214}Bi} component, the higher rate as given by the fitted HPD value was used instead. The same constraints on all the rates were used as in the main analysis. For the energy scale parameter, because no additional systematics were applied to the Asimov dataset the true and expected value of $\alpha$ was set to 1, with the same uncertainty of $\pm0.0022$ as before.

Using this baseline set of assumptions, fake datasets were generated and fit over a series of livetime scenarios between 150 days and 5 years. The results of these MCMC fits, in terms of the 2D and 1D posterior densities for the oscillation parameters are shown in Figs.~\ref{fig:2d_posteriors_projections} and \ref{fig:1d_posteriors_projections}. Some chains took much longer to converge than in the main analysis; as a result, a burn-in of \num{250000} steps was found to be sufficient. As can be seen, by 2 years of livetime, there is sufficient evidence to reject large \dmsq{} values above \SI{2e-4}{\eV\squared} with $3.1\sigma$ of confidence, because the total posterior probability \SI{2e-4}{\eV\squared} is only 0.1\%. This leads to a fairly substantial decrease in uncertainty as well as less bias in the 1D HPD value for \tonetwo{}. There does remain some bias, however: this is because of the non-Gaussian ``boomerang'' structure seen in the 2D posterior density plots.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/steve_style_contours_projections.png}
    \caption[Posterior density posterior distributions marginalised onto the two solar oscillation parameters, for each livetime scenario]
    {Posterior density distributions marginalised onto the two solar oscillation parameters, for each livetime scenario in the pure scintillator phase under the current backgrounds.}
    \label{fig:2d_posteriors_projections}
\end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{6_SolarAnalysis/images/theta_12_comparison_projections.pdf}
%     \caption[Posterior density posterior distributions marginalised onto \tonetwo{}, for each livetime scenario]
%     {Posterior density distributions marginalised onto \tonetwo{}, for each livetime scenario in the pure scintillator phase under the current backgrounds.}
%     \label{fig:1d_posteriors_projections}
% \end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.90\textwidth}
        \centering
        \includegraphics[width=\textwidth]{6_SolarAnalysis/images/theta_12_comparison_projections.pdf}
        \caption{}
        \label{fig:1d_posteriors_projections_theta}
    \end{subfigure}
    \begin{subfigure}{0.90\textwidth}
        \centering
        \includegraphics[width=\textwidth]{6_SolarAnalysis/images/dmsq_21_comparison_projections.pdf}
        \caption{}
        \label{fig:1d_posteriors_projections_dmsq}
    \end{subfigure}
    \caption[Posterior density posterior distributions marginalised onto each of \tonetwo{} and \dmsq{}, for each livetime scenario]
        {Posterior density distributions marginalised onto each of \tonetwo{} and\dmsq{}, for each livetime scenario in the pure scintillator phase under the current backgrounds.}
    \label{fig:1d_posteriors_projections}
\end{figure}

The general story told by these projections is that, assuming the same conditions and analysis, increasing the livetime of the dataset will lead to a substantial improvement to the precision of the measurement of \tonetwo{} made by SNO+, as well as the beginnings of a measurement of \dmsq{}. However, after $\sim$3 years of livetime the rate of progress slows down somewhat.

The choices of assumptions for this baseline set of projections were deliberately chosen to be fairly conservative. However, they do not account for the loading of bis-MSB and Te into the detector that is likely to occur in the coming years. For bis-MSB, the main impact is expected to be a dramatic increase in the observed light yield in the detector, leading to a smaller energy resolution and hence improved measurement~\cite{albaneseSNOExperiment2021}. % cite something
Once the Te is also loaded, the observed light yield will likely decrease back down to levels similar to that of the current LAB-PPO. One of the biggest expected impact of the Te phase on this analysis will instead be to the background levels within the detector: the target rate of \ce{^{238}U}-chain backgrounds in the Te phase is $\sim$100 times that of the scintillator phase~\cite{andringaCurrentStatusFuture2016}. % cite target level backgrounds

Because of this, two further scenarios were considered over a period of 1 year of livetime. In one, the internal \ce{^{238}U}- and \ce{^{232}Th}-chain backgrounds were raised to the nominal level expected during the Te-loaded phase~\cite{kroupovaImprovingSensitivityNeutrinoless2020}. % cite tereza
Unique to the Te phase will be a number of other background processes. These include $2\nu\beta\beta$ decay as well as cosmogenic isotopes created from the spallation of Te nuclei. Because the Q-value of $2\nu\beta\beta$ decay for \ce{^{130}Te} is \SI{2.53}{\MeV}, only a tiny fraction of those events should be expected to make it above this analysis' \SI{2.5}{\MeV} energy threshold. When the cuts described in Table~\ref{tab:ev_selection} were run over the production of $2\nu\beta\beta$ MC made for the dataset used in the above analysis, no events survived. For the cosmogenics, assuming that the purification of the TeLS works as expected, less than one cosmogenic event of any kind is expected in the \onbb{} ROI in a year~\cite{kroupovaImprovingSensitivityNeutrinoless2020}. % cite Tereza
Because of this, both of these additional backgrounds have been ignored for this scenario.

In the other background scenario, the external water backgrounds were set to the lower level measured by T. Zummo in the water phase, and the internal \ce{^{208}Tl} were reduced by a factor of 10. This latter scenario reflects the possibility of better control of the radioactivity in the external water, as well as a way of using new analysis methods to cut out 90\% of the internal \ce{^{208}Tl} events.

The results of running these two scenarios can be seen in Figs.~\ref{fig:2D_posteriors_low_vs_hi_backs}~\&~\ref{fig:1D_posteriors_low_vs_hi_backs}. Unsurprisingly, the $1\sigma$ CI for \tonetwo{} is wider when backgrounds are greater, and thinner in the lower background scenario. However, the magnitudes of the change in width are somewhat different. For the high backgrounds case, the width only increases by 2\%. This implies that the higher internal backgrounds expected during the Te phase should not, on their own, substantially hamper the sensitivity of the measurement of \tonetwo{}. With lower external water and internal \ce{^{208}Tl} backgrounds, it appears that sensitivity improvements $\mathcal{O}(10\%)$ are achievable. This is because, over 1 year of livetime, the statistical uncertainties present in the signal process still dominate.

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/steve_style_contours_projections_low_vs_hi_1yr.png}
    \caption[Posterior densities marginalised onto the two oscillation parameters, for projections over 1 year of livetime, with different background expectations]
    {Posterior densities marginalised onto the two oscillation parameters, for projections over 1 year of livetime, with different background expectations.}
    \label{fig:2D_posteriors_low_vs_hi_backs}
\end{sidewaysfigure}

\begin{figure}[!th]
    \centering
    \includegraphics[width=\textwidth]{6_SolarAnalysis/images/theta_12_comparison_projections_low_vs_Te_backs_1yr.pdf}
    \caption{Posterior densities marginalised onto \tonetwo{}, for projections over 1 year of livetime, with different background expectations.}
    \label{fig:1D_posteriors_low_vs_hi_backs}
\end{figure}

\section{Summary and Suggestions for Further Work}\label{sec:solar_summary}
In this chapter, a full analysis for measuring the solar neutrino oscillation parameters via \beight{} neutrinos was built and demonstrated on SNO+ scintillator-phase data. Using 80.6 days of livetime, assuming the global fit constraint on the \beight{} flux of $\Phi_{\beight{}} = (5.16\,^{+2.5\%}_{-1.7\%})\times 10^{6}\,\si{\per\cm\squared\per\second}$~\cite{bergstromUpdatedDeterminationSolar2016}, % cite Bergstrom et al
the oscillation parameter \tonetwo{} was measured to be $\left|\tonetwo{} + n\cdot180\right|^{\circ} = 38.9^{\circ+8.0^{\circ}}_{\phantom{\circ}-7.9^{\circ}}$.

Furthermore, assuming an identical analysis approach and similar detector conditions to those seen in the above dataset, the precision of this measurement is liable to improve by a factor of 2 in under 2 years of total livetime. This is partly because there will be enough statistics to make a measurement of \dmsq{}.

Of course, major changes to the detector's scintillator cocktail are expected in that timeline. The negative impacts expected from the higher internal backgrounds of the Te-loaded phase should have a small impact on the sensitivity to the oscillation parameters over a livetime of 1 year. The sensitivity obtained in this scenario is expected to be somewhat conservative, as the scintillation emission time profile is expected to become substantially shorter~\cite{autyMethodLoadTellurium2023}, % cite something relevant
leading to substantial improvements in the position resolution of the detector as well as the discrimination power of various time-based classifiers.

If the external water backgrounds can be returned to the levels observed during the water phase, and the event selection of internal \ce{^{208}Tl} events improves substantially, there is some scope for improvement to the measurement of \tonetwo{} when considering 1 year of livetime. There are three main possibilities for removal of internal \ce{^{208}Tl} events. Firstly, these $\beta$-decays come as the delayed partner to the $\alpha$-decay branch of \ce{^{214}Bi} nuclei. Therefore, a coincidence tagging approach to remove these events could be possible. Furthermore, alongside the $\beta$-decay of \ce{^{208}Tl} are typically $\gamma$s, which should modify the observed time residual spectrum of these events compared to a \beight{} signal event. As a result, creating a custom multi-site classifier to exclude internal \ce{^{208}Tl} events from single-site events could help further. Finally, solar neutrino and \ce{^{208}Tl} events can in theory be distinguished by their direction. Via a method such as the one developed in~\cite{allegaEventbyEventDirectionReconstruction2023,patonDirectionalReconstructionLiquid2023}, % cite Josie
it might be possible to gain some discrimination power of the solar signal over this background.

On the subject of classifiers, the choice of cuts used for the externals classifiers were chosen to be deliberately conservative. By tuning those cuts, it is possible that the sensitivity could be improved through the increased rejection of external events. An even more sophisticated approach would be to include these classifiers as dimensions of the PDFs. This method yielded substantial improvements in the \onbb{} study performed in~\cite{kroupovaImprovingSensitivityNeutrinoless2020}.

Of course, what has been seen in this analysis is that the greatest barrier to greater precision is simply the signal statistics. This is a function of exposure, not just livetime. Therefore, substantial improvements are likely to be made by increasing the FV used for this analysis. For example, if the maximum radius used in this analysis was moved from \SI{5.0}{\metre} out to \SI{5.7}{\metre}, then the increased volume leads to an increased rate of signal events of 48\%. Admittedly, for energies below $\sim\SI{3.0}{\MeV}$ at larger radii external backgrounds are expected to completely dominate over the signal, so the benefits in the signal statistics will mostly be seen above \SI{5}{\MeV}.

Because the time of day impacts the extent to which solar neutrinos will be travelling through the Earth, there is expected to be some additional power in considering the difference between events observed during the day versus the night. This approach was used to boost the expected sensitivity for the solar oscillation analysis to be used on the JUNO experiment, for example~\cite{abuslemeFeasibilityPhysicsPotential2020,zhaoModelIndependentApproach2022}. % cite JUNO b8 osc measurement paper(s)
Also considered in that sensitivity study were additional interaction modes of solar neutrinos with a liquid scintillator, such as the CC interaction of $\nu_{e}$ on \ce{^{13}C} within the detector. These modes could provide further sensitivity.

Finally, analysis of neutrino oscillations from reactor anti-neutrinos in SNO+ is expected to lead to a world-leading limit of \dmsq{} in under 4 years of livetime~\cite{morton-blakeFirstMeasurementReactor2021}. Although both analysis approaches have sensitivity to \dmsq{} and \tonetwo{}, the reactor anti-neutrino method is more sensitive to \dmsq{} whilst the solar analysis shown above is more sensitive to \tonetwo{}. Because of this complementarity, it is possible that a combined analysis using both types of oscillating neutrino signal incident on SNO+ will lead to even better measurements of both oscillation parameters.


% {
%     \color{blue}
%     \begin{itemize}
%         \item Using the same production MC, generate PDFs with the expected normalisations for longer periods of livetime: 1, 3, and 5 years. This still assumes a scintillator-fill with identical detector conditions on average, so not considering the impact of bis-MSB loading at any point.
%         \item Describe any further constraints to be assumed on top of the existing analysis of data: expected improved constraints on various backgrounds, as well as a possible energy-scale calibration constraint from the AmBe source or an internally-deployed source, for example.
%         \item Run MCMC fits to these Asimov PDFs for each livetime scenario. Describe results in terms of the improvement to the sensitivity to \tonetwo{} as a function of livetime.
%         \item Also consider scenario of lower backgrounds! How does that impact the result?
%         \item Could consider scenario of bis-MSB deployment, leading to substantially greater light yield and hence energy resolution. Given time it would take for Production MC to be generated for this scenario, I would likely have to come up with something clever and quick to actually do this. There quite possibly won't be enough time.
%     \end{itemize}
%     [8 pages]

%     [61 PAGES TOTAL]
% }